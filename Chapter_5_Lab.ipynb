{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 Lab: Cross-Validation and the Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import pandas as pd \n",
    "import math\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.graphics.regressionplots import *\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from collections import OrderedDict\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "sns.set_style(\"ticks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.1 The Validation Set Approach\n",
    "\n",
    "We explore the use of the validation set approach in order to estimate the test error rates that result from fitting various linear models on the `Auto` data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>year</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23.445918</td>\n",
       "      <td>5.471939</td>\n",
       "      <td>194.411990</td>\n",
       "      <td>104.469388</td>\n",
       "      <td>2977.584184</td>\n",
       "      <td>15.541327</td>\n",
       "      <td>75.979592</td>\n",
       "      <td>1.576531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.805007</td>\n",
       "      <td>1.705783</td>\n",
       "      <td>104.644004</td>\n",
       "      <td>38.491160</td>\n",
       "      <td>849.402560</td>\n",
       "      <td>2.758864</td>\n",
       "      <td>3.683737</td>\n",
       "      <td>0.805518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>1613.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>2225.250000</td>\n",
       "      <td>13.775000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>22.750000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>93.500000</td>\n",
       "      <td>2803.500000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>275.750000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>3614.750000</td>\n",
       "      <td>17.025000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>46.600000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>455.000000</td>\n",
       "      <td>230.000000</td>\n",
       "      <td>5140.000000</td>\n",
       "      <td>24.800000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              mpg   cylinders  displacement  horsepower       weight  \\\n",
       "count  392.000000  392.000000    392.000000  392.000000   392.000000   \n",
       "mean    23.445918    5.471939    194.411990  104.469388  2977.584184   \n",
       "std      7.805007    1.705783    104.644004   38.491160   849.402560   \n",
       "min      9.000000    3.000000     68.000000   46.000000  1613.000000   \n",
       "25%     17.000000    4.000000    105.000000   75.000000  2225.250000   \n",
       "50%     22.750000    4.000000    151.000000   93.500000  2803.500000   \n",
       "75%     29.000000    8.000000    275.750000  126.000000  3614.750000   \n",
       "max     46.600000    8.000000    455.000000  230.000000  5140.000000   \n",
       "\n",
       "       acceleration        year      origin  \n",
       "count    392.000000  392.000000  392.000000  \n",
       "mean      15.541327   75.979592    1.576531  \n",
       "std        2.758864    3.683737    0.805518  \n",
       "min        8.000000   70.000000    1.000000  \n",
       "25%       13.775000   73.000000    1.000000  \n",
       "50%       15.500000   76.000000    1.000000  \n",
       "75%       17.025000   79.000000    2.000000  \n",
       "max       24.800000   82.000000    3.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Auto = pd.read_csv('https://www.statlearning.com/s/Auto.csv', header=0, na_values='?')\n",
    "Auto = Auto.dropna().reset_index(drop=True) # drop the observation with NA values and reindex the obs from 0\n",
    "Auto.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, we use the `random_state=42` parameter in order to set a seed for `python`'s random number generator, this is so that the user of this lab will obtain precisely the same results as those in the discussions below. \n",
    "\n",
    "It is generally a good idea to set a random seed when performing an analysis such as cross-validation that contains an\n",
    "element of randomness, so that the results obtained can be reproduced precisely at a later time.\n",
    "\n",
    "We begin by using the `train_test_split()` function to split the set of observations into two halves, by selecting a random subset of $196$ observations out of the original $392$ observations. We refer to these observations as the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the `Train` to fit a linear regression using only the observations corresponding to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and record the index of train samples\n",
    "Train, Valid = train_test_split(Auto, test_size=196, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    mpg   R-squared:                       0.614\n",
      "Model:                            OLS   Adj. R-squared:                  0.612\n",
      "Method:                 Least Squares   F-statistic:                     308.5\n",
      "Date:                Thu, 20 Oct 2022   Prob (F-statistic):           5.83e-42\n",
      "Time:                        14:28:32   Log-Likelihood:                -592.23\n",
      "No. Observations:                 196   AIC:                             1188.\n",
      "Df Residuals:                     194   BIC:                             1195.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     40.8582      1.036     39.431      0.000      38.815      42.902\n",
      "horsepower    -0.1648      0.009    -17.565      0.000      -0.183      -0.146\n",
      "==============================================================================\n",
      "Omnibus:                       10.932   Durbin-Watson:                   2.173\n",
      "Prob(Omnibus):                  0.004   Jarque-Bera (JB):               11.512\n",
      "Skew:                           0.593   Prob(JB):                      0.00316\n",
      "Kurtosis:                       3.069   Cond. No.                         321.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# start to build the model\n",
    "lm = smf.ols ('mpg~horsepower', data=Train).fit()\n",
    "print(lm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we choose a different training set instead, then we will obtain somewhat different errors on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now  use\n",
    " the `predict()` function to estimate the response for all $392$ observations, and we  use the `mean()` function to calculate the MSE of the $196$ observations in the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Test error for 1st order model --------\n",
      "23.442643969985753\n"
     ]
    }
   ],
   "source": [
    "# to follow the book, get prediction for all the observations in the dataset\n",
    "# here we use ~ select to exclude the result of the training samples\n",
    "lm = smf.ols ('mpg~horsepower', data=Train).fit()\n",
    "preds = lm.predict(Valid)\n",
    "square_error = (Valid['mpg'] - preds)**2\n",
    "print('-------- Test error for 1st order model --------')\n",
    "print(np.mean(square_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the estimated test MSE for the linear regression fit is $25.57$. We can use the `I(X**2.0)` and `I(X**3.0)` function to estimate the test error for the quadratic and cubic regressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Test error for 2nd order--------\n",
      "18.550198801910682\n"
     ]
    }
   ],
   "source": [
    "# build a model with 2nd order features  \n",
    "lm2 = smf.ols ('mpg~horsepower + I(horsepower ** 2.0)', data=Train).fit()\n",
    "preds = lm2.predict(Valid)\n",
    "square_error = (Valid['mpg'] - preds)**2\n",
    "print('--------Test error for 2nd order--------')\n",
    "print(square_error.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Test rror for 3rd order--------\n",
      "18.595222294405268\n"
     ]
    }
   ],
   "source": [
    "# build a model with 3rd order features  \n",
    "lm3 = smf.ols ('mpg~horsepower + I(horsepower ** 2.0) + I(horsepower ** 3.0)', data=Train).fit()\n",
    "preds = lm3.predict(Valid)\n",
    "square_error = (Valid['mpg'] - preds)**2\n",
    "print('--------Test rror for 3rd order--------')\n",
    "print(np.mean(square_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this split of the observations into a training set and a validation set, we find that the validation set error rates for the models with linear, quadratic, and cubic terms are $23.44$, $18.55$, and $18.59$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the summmary for 3rd order regression, the coefficient of the 3rd order term is not statistically significant. We will use this as Supporting evidence for the above claim. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    mpg   R-squared:                       0.695\n",
      "Model:                            OLS   Adj. R-squared:                  0.690\n",
      "Method:                 Least Squares   F-statistic:                     145.5\n",
      "Date:                Thu, 20 Oct 2022   Prob (F-statistic):           3.30e-49\n",
      "Time:                        14:28:33   Log-Likelihood:                -569.28\n",
      "No. Observations:                 196   AIC:                             1147.\n",
      "Df Residuals:                     192   BIC:                             1160.\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "========================================================================================\n",
      "                           coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------\n",
      "Intercept               56.7613      6.304      9.004      0.000      44.328      69.195\n",
      "horsepower              -0.4502      0.165     -2.736      0.007      -0.775      -0.126\n",
      "I(horsepower ** 2.0)     0.0011      0.001      0.792      0.429      -0.002       0.004\n",
      "I(horsepower ** 3.0)   4.92e-07   3.37e-06      0.146      0.884   -6.16e-06    7.14e-06\n",
      "==============================================================================\n",
      "Omnibus:                        9.793   Durbin-Watson:                   2.062\n",
      "Prob(Omnibus):                  0.007   Jarque-Bera (JB):               13.839\n",
      "Skew:                           0.314   Prob(JB):                     0.000988\n",
      "Kurtosis:                       4.141   Cond. No.                     5.13e+07\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 5.13e+07. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "print(lm3.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are consistent with our previous findings: \n",
    "\n",
    "a model that predicts `mpg` using a quadratic function of `horsepower` performs better than a model that involves only a linear function of `horsepower`, and there is little evidence in favor of a model that uses a cubic function of `horsepower`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.2 Leave-One-Out Cross-Validation\n",
    "The LOOCV estimates only keep one sample in the validation data and use the rest of the data to train the model. This way the training model has similar dataset comparing to the model trained on entire dataset.\n",
    "\n",
    "The LOOCV estimate can be automatically computed for any generalized linear model using the `ols()` and `cross_val_score()` functions.  In the lab for Chapter 4, we used the `logit()` function to perform logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept     39.935861\n",
      "horsepower    -0.157845\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# OLS fit \n",
    "ols_fit = smf.ols('mpg~horsepower', data=Auto).fit()\n",
    "print(ols_fit.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept     39.935861\n",
      "horsepower    -0.157845\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# GLM fit. Compare with OLS fit, the coeffs are the same\n",
    "glm_fit = smf.glm('mpg~horsepower', data=Auto).fit()\n",
    "print(glm_fit.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trying CV in Python is not as easy as that in R. It will require some manual coding.\n",
    "# to use some of implemented function in Python, we use Sklearn for linear model \n",
    "# from sklearn.model_selection import KFold, cross_val_score\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.93586102117045\n",
      "[-0.15784473]\n"
     ]
    }
   ],
   "source": [
    "# let us re-train the model in sklearn\n",
    "model = LinearRegression()\n",
    "model.fit(Auto.horsepower.to_frame(), Auto.mpg)\n",
    "print(model.intercept_)\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yield identical linear regression models. In this lab, we will  perform linear regression using  the `glm()` and `LinearRegression()` rather than the `ols()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOOCV use folds equal to # of observations. We could also choose other number of folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.23151351792923\n"
     ]
    }
   ],
   "source": [
    "n_split = Auto.shape[0]\n",
    "test = cross_val_score(model, \n",
    "                       Auto.horsepower.to_frame(),\n",
    "                       Auto.mpg, \n",
    "                       cv=Auto.shape[0],  \n",
    "                       scoring='neg_mean_squared_error',                        \n",
    "                       n_jobs=-1)\n",
    "print(np.mean(-test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat this procedure for increasingly complex polynomial fits.\n",
    "To automate the process, we use the  `for` loop to initiate a  which iteratively fits polynomial regressions for polynomials of order $i=1$ to $i=10$, computes the associated cross-validation error.\n",
    "We begin by initializing the vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for higher order polynomial fit, we use pipline tool.\n",
    "# this step may take a few mins\n",
    "A = OrderedDict()\n",
    "n_split = Auto.shape[0]\n",
    "for porder in range(1, 11, 1):\n",
    "    model = Pipeline([('poly', PolynomialFeatures(degree=porder)), ('linear', LinearRegression())])    \n",
    "    test = cross_val_score(model, \n",
    "                           Auto.horsepower.to_frame(),\n",
    "                           Auto.mpg, \n",
    "                           cv=n_split,  \n",
    "                           scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    A[str(porder)] = np.mean(-test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('1', 24.231513517929226),\n",
       "             ('2', 19.248213124489393),\n",
       "             ('3', 19.334984064131373),\n",
       "             ('4', 19.4244303095644),\n",
       "             ('5', 19.033216775741238),\n",
       "             ('6', 18.97488351827742),\n",
       "             ('7', 19.12593012097048),\n",
       "             ('8', 19.224202151694772),\n",
       "             ('9', 19.13396063366187),\n",
       "             ('10', 18.946435735916353)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(A.values(), columns=['MSE'], index=pd.Index(A.keys(), name='Flexibility')).plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in Figure 5.4, we see a sharp drop in the estimated test MSE between the linear and quadratic fits, but then no clear improvement from using higher-order polynomials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.3 k-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Below we use $k=10$, a common choice for $k$, on the `Auto` data set.\n",
    "We once again set a random seed and initialize a vector in which we will store the CV errors corresponding to the polynomial fits of orders one to ten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold validation is exactly same as LOO with different n_splits parameter setup. \n",
    "# the computation time is much shorter than that of LOOCV.\n",
    "np.random.seed(2)\n",
    "A = OrderedDict()\n",
    "n_split = 10\n",
    "for porder in range(1, 11, 1):\n",
    "    model = Pipeline([('poly', PolynomialFeatures(degree=porder)), ('linear', LinearRegression())])    \n",
    "    test = cross_val_score(model,\n",
    "                           Auto.horsepower.to_frame(),\n",
    "                           Auto.mpg, \n",
    "                           cv=n_split,  \n",
    "                           scoring='neg_mean_squared_error',                           \n",
    "                           n_jobs=-1)\n",
    "    A[str(porder)] = np.mean(-test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('1', 27.439933652339857),\n",
       "             ('2', 21.235840055802118),\n",
       "             ('3', 21.336606183328794),\n",
       "             ('4', 21.35388699422229),\n",
       "             ('5', 20.90563054316467),\n",
       "             ('6', 20.799631258355426),\n",
       "             ('7', 20.953366101157698),\n",
       "             ('8', 21.077180808300973),\n",
       "             ('9', 21.036717405054862),\n",
       "             ('10', 20.980983108692712)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEMCAYAAADUEk3/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAmOklEQVR4nO3df1Rbd/0/8Oe9CUkIcBN+kxSaxK6t3bqNOixbx751dHXqd7Wlnh1RUObE6TZ1utNT/W7qTunajoqyKdV8q9s5/c7KV/30x4qbH1qtO9rq2s65KdtXhq6FUkhXKBB+B5L7/SMQSEubhF+X3Dwf5/SQvJObvMjZnnnzvu/7fguyLMsgIqKoJypdABERzQ4GOhGRSjDQiYhUgoFORKQSDHQiIpXQKvXGQ0NDaGhoQHp6OjQajVJlEBFFFa/Xi0uXLmHlypUwGAxBjykW6A0NDSgpKVHq7YmIotr+/fuRl5cX1KZYoKenpwPwF5WVlaVUGUREUcXlcqGkpCSQoZMpFujjwyxZWVnIzs5Wqgwioqg01VA1T4oSEakEA52ISCUUG3IhIgqHz+dDa2sr+vv7lS5lXiUkJCA7OxuiGH6/m4FORAtaR0cHBEHA8uXLIwq3aObz+XDhwgV0dHQgIyMj7ONi49MhoqjV3d2NzMzMmAlzABBFEZmZmejp6YnsuDmqh4hoVni9XsTFxSldxryLi4vD6OhoRMdEZaA/+3/fwP7//pfSZRDRPBEEQekS5t10fueoDPTu3mG81tCudBlEFIMKCwtRUFAAr9cbaDtw4ACWL1+OX/ziF+jp6cHjjz+O++67Dxs2bMDGjRvx17/+FQBw8OBB5OXlYePGjYF/VVVVs1ZbVJ4UdVhNePPdSxgZ9SJOy3VgiGh+paen48SJE1i7di0A4PDhw7jpppsAAM8++ywyMzPxgx/8AIIgoKurC4ODg4Fj16xZgx/96EdzUldU9tAdVglen4zW9/uULoWIYlBRUREOHjwIADh//jwGBwexbNkyAP5L8zMzMwNDJsnJybBarfNSV9T20AHgbFtP4DYRqd/x11tw7HTLnLz2+tWLUZi3OKzn5ufn45e//CV6enpw6NAhbNq0CQ0NDQCAz3/+8/j617+O3/72t1i1ahUKCwtxxx13BI79y1/+go0bNwbul5aW4v7775+V3yEqA92algCdVsTZNrfSpRBRDBIEAR//+Mfx8ssv45VXXkFtbW0g0O+44w788Y9/xKlTp/C3v/0N3/jGN/DFL34RDz30EIC5HXKJykDXaEQstkg42xbZHE0iim6FeeH3oufa5s2bcf/992P16tVITk4OeiwxMRHr1q3DunXrsHLlSvz0pz8NBPpcisoxdABwWCScbXNDlmWlSyGiGJSTk4NvfvObeOSRR4LaT548ib4+//k9WZbxzjvvzNuKslHZQwcAu1XCsdMtuOweQqopXulyiCgGffrTn76qrbGxEc8880ygs2mz2fC9730v8PiVY+grV67Ejh07ZqWeqA30iROjbgY6Ec2b48ePT9n+zDPPBG4/+OCDUz5n8+bN2Lx585zUBYQR6F1dXdi6dStaWlqg0+lgs9lQUVGBgYEBPProo4Hn9fb2oq+vD6dPn56zYidzWCQA/pkueSsy5+U9iYgWspCBLggCysvLkZ+fDwCorKxEVVUVdu7ciZdeeinwvB07dgRdOTXXEo06pCfH41w7Z7oQEQFhnBQ1m82BMAeA3NxctLW1BT3H4/Ggrq4On/rUp2a/wutwWEycukhENCaiMXSfz4fa2loUFhYGtR8/fhyZmZmBS1+v5Ha74XYHB6/L5Yqw1Ks5rBJe/9dFeEa80MVxCQAitZJlOeYW6JrODL6IAn379u0wGo0oLS0Naj9w4MB1e+f79u1DTU1NxMWF4rCa4PPJaHH14oYc86y/PhEpz2AwoLOzE6mpqTET6rIso7OzEwaDIaLjwg70yspKNDc3w+l0Bi00f/HiRZw5cwa7d+++5rFlZWUoKioKanO5XCgpKYmo2CvZrRMnRhnoROqUnZ2N1tZWXLp0SelS5pXBYIh4/npYgV5dXY2Ghgbs3bsXOp0u6LFDhw5h7dq1V10pNZkkSZAkKaLCwpGVmgC9ToOzPDFKpFpxcXFwOBxKlxEVQgZ6U1MTnE4n7HY7iouLAfi/Mffs2QPAH+hPPvnk3FZ5DRpRgD2LSwAQEQFhBPrSpUvR2Nh4zcfr6+tntaBI2a0STrzVFpMnTYiIJovatVzGOawm9A+OoKN7SOlSiIgUpYJAHzsx2s5hFyKKbVEf6PZJSwAQEcWyqA90oyEOWalGXjFKRDEv6gMd8I+jn2MPnYhinCoC3W6R0NbRj6HhUaVLISJSjCoC3WGVIMtAs4vDLkQUu1QS6BObXRARxSpVBHpGshHxei3XRieimKaKQBdFAXYLlwAgotimikAH/OPo59rd01pDmIhIDVQU6CYMDI3i4uUBpUshIlKEigJ9/IpRjqMTUWxSTaDbsiQIAniBERHFLNUEukGvhSU1gZtdEFHMUk2gA/5xdM50IaJYpbJAl+DqHMDA0IjSpRARzTuVBbr/itHm9l6FKyEimn+qCnQ7N7sgohimqkBPN8cjIT6OUxeJKCapKtAFQYDDyiUAiCg2aUM9oaurC1u3bkVLSwt0Oh1sNhsqKiqQkpKC4eFh7Ny5E3/961+h1+uRm5uL7du3z0fd12S3SPj96Rb4fDJEUVC0FiKi+RQy0AVBQHl5OfLz8wEAlZWVqKqqws6dO/H9738fer0e9fX1EAQBHR0dc15wKA6rCUMeL1yd/bCmJypdDhHRvAk55GI2mwNhDgC5ubloa2tDf38/Dh8+jMceewyC4O8Jp6WlzV2lYeISAEQUq0L20Cfz+Xyora1FYWEhzp8/D7PZjJqaGpw6dQoJCQl47LHHkJeXd9VxbrcbbndwwLpcrplVfg2LsySIgn+my523WufkPYiIFqKIAn379u0wGo0oLS3FO++8g/Pnz+PGG2/Et771Lbz11lv4yle+gmPHjiExMXioY9++faipqZnVwq9FH6fBooxEnGMPnYhiTNiBXllZiebmZjidToiiCKvVCq1Wi/vuuw8AcOuttyI5ORlnz57FzTffHHRsWVkZioqKgtpcLhdKSkpm4Ve4msNiwr+aL8/JaxMRLVRhBXp1dTUaGhqwd+9e6HQ6AEBKSgry8/Nx8uRJFBQU4OzZs+js7ITNZrvqeEmSIEnS7FZ+HXarhD+9eQF9gyNIjI+bt/clIlJSyEBvamqC0+mE3W5HcXExACA7Oxt79uzBtm3b8MQTT6CyshJarRa7d++e1+C+lvElAM619WDlEuVP1BIRzYeQgb506VI0NjZO+VhOTg5efPHFWS9qpibPdGGgE1GsUNWVouNSJAOSjDpeMUpEMUWVgR5YAoCbXRBRDFFloAP+cfSWdje8Xp/SpRARzQsVB7oEz6gPbR39SpdCRDQvVBzo4zNdOOxCRLFBtYGek5kIjShwswsiihmqDfQ4rQY5mUlcpIuIYoZqAx3wXzHKqYtEFCtUHegOi4TOniG4+z1Kl0JENOdUHej2sROj7KUTUSxQdaBzswsiiiWqDvTkJAPMSXqc40wXIooBqg50wD+Ozh46EcUC9Qe61YQWVy9GuQQAEalcDAS6hFGvDxfe71O6FCKiORUDgc6ZLkQUG1Qf6IsyEqHViBxHJyLVU32gazUiFmcmsYdORKqn+kAHxpYA4GYXRKRyMRHoDqsJ3b3D6OodUroUIqI5EyOB7r9ilGujE5GaaUM9oaurC1u3bkVLSwt0Oh1sNhsqKiqQkpKCwsJC6HQ66PV6AMCWLVtw1113zXnRkZqY6eLGquUZCldDRDQ3Qga6IAgoLy9Hfn4+AKCyshJVVVXYuXMnAOBHP/oRli1bNrdVzpCUoEOqycDNLohI1UIGutlsDoQ5AOTm5qK2tjaiN3G73XC7g4c7XC5XRK8xUw6riUMuRKRqIQN9Mp/Ph9raWhQWFgbatmzZAlmWcdttt+Hxxx+HJElXHbdv3z7U1NTMvNoZsFsk/L3xfYyMehGn1ShaCxHRXIgo0Ldv3w6j0YjS0lIAwP79+2GxWODxeLBjxw5UVFSgqqrqquPKyspQVFQU1OZyuVBSUjKD0iPjsErw+mScv9iHDywyzdv7EhHNl7ADvbKyEs3NzXA6nRBF/+QYi8UCANDpdPjsZz+Lhx9+eMpjJUmasuc+nyYvAcBAJyI1CivQq6ur0dDQgL1790Kn0wEABgYG4PV6kZSUBFmW8corr2DFihVzWuxMWNMSoNOKOMcLjIhIpUIGelNTE5xOJ+x2O4qLiwEA2dnZ+Pa3v42vfe1r8Hq98Pl8WLJkCZ566qk5L3i6NBoRiy3cNJqI1CtkoC9duhSNjY1TPnb48OHZrmdOOSwSTr3tgizLEARB6XKIiGZVTFwpOs5hNcHd78FlN5cAICL1ibFA56bRRKReMRXodm52QUQqFlOBnhgfh/TkeF4xSkSqFFOBDgAOi4lruhCRKsVeoFslXHi/D54Rr9KlEBHNqhgMdBN8MtDi6lW6FCKiWRWDgT4+04XDLkSkLjEX6FmpCTDoNNxjlIhUJ+YCXRQF2LgEABGpUMwFOuAfRz/b5oYsy0qXQkQ0a2Iy0O0WCf2DI7jUPah0KUREsyYmA338xCgvMCIiNYnJQLdbxma68AIjIlKRmAx0oyEOWalGLtJFRKoSk4EO+E+MnuNMFyJSkdgNdIuEto5+DA2PKl0KEdGsiNlAt1tNkGWg2cVhFyJSh5gNdG52QURqE7OBnpFsRLxeyytGiUg1QgZ6V1cXvvSlL+Hee+/Fhg0b8NWvfhWXL18Oek5NTQ2WL1+Od999d84KnW2iKMBukdhDJyLVCBnogiCgvLwc9fX1qKurQ05ODqqqqgKPv/3223jzzTdhtVrntNC54LBKONfOJQCISB1CBrrZbEZ+fn7gfm5uLtra2gAAHo8HFRUVeOqppyAIwtxVOUccVhMGh0dx8fKA0qUQEc2YNpIn+3w+1NbWorCwEADw3HPP4ZOf/CRycnKue5zb7YbbHTy04XK5Iix19k0+MZqVmqBwNUREMxNRoG/fvh1GoxGlpaX4+9//jn/+85/YsmVLyOP27duHmpqaaRc5V2xZEgQBONfWgztutihdDhHRjIQd6JWVlWhubobT6YQoijhz5gzee+89rFu3DoC/x/3FL34Ru3btQkFBQdCxZWVlKCoqCmpzuVwoKSmZhV9h+gx6LaxpCdzsgohUIaxAr66uRkNDA/bu3QudTgcAeOihh/DQQw8FnlNYWAin04lly5ZddbwkSZAkaZZKnl12qwn/ae1WugwiohkLGehNTU1wOp2w2+0oLi4GAGRnZ2PPnj1zXtx8cFgknHyrDQNDIzAa4pQuh4ho2kIG+tKlS9HY2BjyhY4fPz4rBc03h9UEADjX7saNjlSFqyEimr6YvVJ0nH18swuOoxNRlIv5QE83xyMhPo5XjBJR1Iv5QBcEAQ6rxDVdiCjqxXygA/5x9OZ2N3w+LgFARNGLgQ7/TJchjxeuzn6lSyEimjYGOiZmunAcnYiiGQMdwOKsJIgCOI5ORFGNgQ5AF6fBooxE9tCJKKox0Mc4LCaca2cPnYiiFwN9jN0q4f2uQfQNjihdChHRtDDQxwSWAOA4OhFFKQb6mMmbXRARRSMG+pgUyQApQceZLkQUtRjoYwJLAHCRLiKKUgz0SRxWE1ra3fB6fUqXQkQUMQb6JHaLBM+oD20dXAKAiKIPA32SiZkuHHYhoujDQJ8kJzMRGlHAWV5gRERRiIE+SZxWg5zMJE5dJKKoxEC/gp2bXRBRlGKgX8FhMaGzZwjufo/SpRARRUQb6gldXV3YunUrWlpaoNPpYLPZUFFRgZSUFDzyyCNobW2FKIowGo347ne/ixUrVsxH3XNm4orRHty6NF3haoiIwheyhy4IAsrLy1FfX4+6ujrk5OSgqqoKAFBZWYkjR47g8OHDePDBB/HEE0/MecFzjZtdEFG0ChnoZrMZ+fn5gfu5ubloa2sDACQlJQXa+/r6IAjCHJQ4v8xJepiT9BxHJ6KoE3LIZTKfz4fa2loUFhYG2p588kmcPHkSsizj5z//+ZTHud1uuN3BPV6XyzWNcueHwyJxLjoRRZ2IAn379u0wGo0oLS0NtO3YsQMAcPjwYezevRs/+9nPrjpu3759qKmpmWGp88dhNeHIn9/DqNcHrYbnjYkoOoQd6JWVlWhubobT6YQoXh1ymzZtwve+9z10dXUhOTk56LGysjIUFRUFtblcLpSUlEyz7LnlsEoY9fpw4f0+2CyS0uUQEYUlrECvrq5GQ0MD9u7dC51OBwDo7++H2+2GxWIBABw/fhwmkwlms/mq4yVJgiRFTzBOnBjtYaATUdQIGehNTU1wOp2w2+0oLi4GAGRnZ2Pbtm147LHHMDg4CFEUYTKZ4HQ6VXFidFFGIrQaEWfb3PjIbUpXQ0QUnpCBvnTpUjQ2Nk752K9//etZL2gh0GpELM5K4kwXIooqPON3DdzsgoiiDQP9GhxWE7p7h9HVO6R0KUREYWGgX4Pdwk2jiSi6MNCvgZtdEFG0YaBfg5SgQ6rJwM0uiChqMNCvw2E1sYdORFGDgX4dDquE8xd7MTLqVboUIqKQGOjX4bCY4PXJOH+xT+lSiIhCYqBfh33SZhdERAsdA/06rOmJ0GlFTl0koqjAQL8OjShgsYWbRhNRdGCgh+CwSDjb5oYsy0qXQkR0XQz0EBxWE3oHPLjs5hIARLSwMdBDcFi5BAARRQcGegj2SZtdEBEtZAz0EBLj45CRHM8rRolowWOgh8FhNXFNFyJa8BjoYbBbJVx4vw/DI1wCgIgWLgZ6GBwWE3wy0OLisAsRLVwM9DCMz3ThODoRLWQM9DBkpSbAoNNwj1EiWtC0oZ7Q1dWFrVu3oqWlBTqdDjabDRUVFRAEYcr2lJSU+ah7XomiABuXACCiBS5kD10QBJSXl6O+vh51dXXIyclBVVXVNdvVymE1cQkAIlrQQga62WxGfn5+4H5ubi7a2tqu2a5WDquE/sERXOoeVLoUIqIphRxymczn86G2thaFhYVhtY9zu91wu4PHn10uV4SlKsthmdg0OiPZqHA1RERXiyjQt2/fDqPRiNLS0rDax+3btw81NTXTr3IBsFmSAPiXAFh9U5bC1RARXS3sQK+srERzczOcTidEUQzZPllZWRmKioqC2lwuF0pKSqZZ9vwzGuKQlWrkIl1EtGCFFejV1dVoaGjA3r17odPpQrZfSZIkSJI082oV5rCacI5LABDRAhUy0JuamuB0OmG321FcXAwAyM7Oxje+8Y0p2/fs2TO3FSvIYZHwWkM7hoZHYdBHNFpFRDTnQqbS0qVL0djYOOVj12pXK7vVBFkGml1uLLepb749EUU3XikaAW52QUQLGQM9ApkpRhgNWl4xSkQLEgM9AoIgwD62aTQR0ULDQI+Qf6aLGz4flwAgooWFgR4hh1XC4PAo3u8aULoUIqIgDPQI2S08MUpECxMDPUK2LAmCAJzjiVEiWmAY6BEy6LWwpiVwswsiWnAY6NNgt5o4dZGIFhwG+jQ4rBJcnQMYGBpRuhQiogAG+jQ4rGNro3PYhYgWEAb6NIxvdsGZLkS0kDDQpyHNbEBifBzH0YloQeEasNMgCALsVkmxIRdZljE4PIruvmF09w7D3e/BqNcHr1eG1yfD5/PB6/PfvrLNN97uk+H1Tt3mk8eO8Y63T7ze5LbJx/l8MnRxIgpuXYTCvByYEvWKfDZEsYyBPk0OqwnHTjXD55MhisKMX8/nk9E74EFP33AgqAM/x273TLrvGfXN6P00ogCNKEAM/BSh0QiBdo0o+h/TBD9vvF2rEaGJE6DRiIHHO3sG8ULd2/g/r7yD21dacO/tNtxyQ/qsfD5EFBoDfZocFglDHi9cnf2wpidO+ZxRry8Qwj19HnT3DY2FswfdvUNBQd3T54F3ivVhRFGAOVEHc6IB5iQ9FqUnwpxkgDlRD3OSHuZEPaREHXRaMShcJ36KEyGt8Qe3KPj/ypgLze1uHD3VjOOvn8eJt9qQlWrE+tU23LN6MVIkw5y8JxH5MdCnaXymy5E/v4dkST8R2r3DgeDuHZh6WqNOK/rDOEmPdLMRN2SbA/eTEw0wJenGAts/Vh9NPVybRcKXNt2Msv95I/7yz3Ycfa0ZL/7u/2F//b/w4RWZuPd2Gz70wUxoouh3IooWDPRpWpyVhHi9Fi+fPAsAMBq0gV5zTmYSVi5JQ/J4LzpJD9OkHnW8XjtnPeSFQhenwUc+lI2PfCgbbZf6cPRUM/5w5jxOve1CmsmAe1bbsD5/MTKSjUqXSqQaDPRp0sVp8L//1zqMjPpgTtRDF6dRuqQFy5qeiAfuuwklH1uB0++4cPS1Zvzq94341e8bsWp5Bu7Nt2H1TVnQajjpimgmGOgzkJzEMeFIxGlF3HmLFXfeYsXFywM4droZvz/dgl37zsCcpMc9H16M9fmLYU2b+pwEEV0fA50UkZliROnHVuAz65fjb43v4+hrzTj46r/xX8ebcMsNafhovg1rbrEgTsu/fIjCFTLQu7q6sHXrVrS0tECn08Fms6GiogIpKSmorKxEfX09Lly4gLq6Oixbtmw+aiYV0WhErL4xC6tvzEJnzyB+f6YFR0+1oGr/35B0KA535+Xg3nwbFmdJSpdKYRoaHkWnewgd3YPo7BlER/cQutxDEEQBOq0IfZwGukn/9HHixG2dZtLjVzxXK6r+3NNMhQx0QRBQXl6O/Px8AEBlZSWqqqqwc+dOrFu3Dp///OdRUlIy54WS+qWa4vHpe5bj/sJleKvpEupPNeOVk2dx5E/vYYU9BR/Nt6Eg1wqDjn9YKkGWZfQPjaKzZxCd3UPo6BlEZ/cgOnqCb/cPXj27KyE+DgAw7PFi1Dv9ayh0WhF63eQvA3/wB93Xjn0ZBH05jH9BaBGv18Cg1yL+in8GnQYGnTaqZpVdKeT/GWazORDmAJCbm4va2loAQF5eXlhv4na74XYHX1XpcrkiqZNiiCgKWLU8A6uWZ6C7dxjHXz+Po6fO4blf/R0/e+mf+MiHsnHv7XZ8YJFJ6VJVQ5ZluPs96LwinCf3sjt7BjHk8V51bHKSHqkmA7JSE7BySRpSTQakmeORZopHqtmAVFM89JMmDXh9MkZGvBge8cIz4oNn1AvP2P1hj/+2Z8Q39vjEY1O3TbT39nvQEfS4D54RL0YivAjPoNNMhPwUoR9v0CJeN9Zm0MIwfluvCTrGOHZ7Pk/2R9TV8fl8qK2tRWFhYURvsm/fPtTU1ER0DBEAmJP02Hz3DSj6yBK8/V4n6k8149jpFrzyl3O4IceMe/Nt+B+rFsFoiFO61AXL65PR3TuEzp6hoHDuGO9l9wyis2foquATRQEpSXqkmuNht0i4bUUG0kwTQZ1mikeyZECcNrLA0ogCNGNhNx98PnnsS8OHIc8ohj1eDA6PBv4NBW7724c8Y/eHRjE4dru7dwjtHcHPkcPcJz5OK/pD36BF/NiXxdLFyXho082z/rtG9Ilu374dRqMRpaWlEb1JWVkZioqKgtpcLheHaihsgiBg5ZI0rFyShi9vuhl//Fsrjp5qxp7/egvPH2nAXbmLcO/tNixbnByT46x9Ax64OgfQ3tkPV2c/XJ0DYz/70dEzBN8VVyFrNWKgJ71scXJQSKeZ45FqMsCcZFDFBWCiKMCg08KgA6QE3ay8pizL/i8Gz/iXQvCXRPAXxdj9SV8kozNcuuNawg70yspKNDc3w+l0QhQj+0aWJAmSxJNaNDsSjTpsuOsDuK/AgXdbulD/WjP+/OYFHDvdArtFwp23WpFujvdfeZtkQHKSHlKiPqrDyeuT0dkzCFdnP9o7BnDxcj/aOybCu++KcWtzoh6ZqUassKciIyU+MASSYvKHtpSgi+qxYqUJggDD2F8ZyUlKVzMhrECvrq5GQ0MD9u7dC51udr7hiGZKEAQst6VguS0F5RtX4k9/v4D6U83Y/9//uuq5ogBIiXokj4W8P+z9yyskJ+mRLE2Ef0J8nCK9/CHPKC6O9azbJ/WwXZ39uHh5MOhkokYUkJFsRFaqEUtzkpGVmgBLmhFZqQnITDFyCCpGhQz0pqYmOJ1O2O12FBcXAwCys7OxZ88ePP300zh69Cg6OjrwhS98AWazGS+//PKcF010JaMhDh+7w46P3WEfG/P0L4rW1TuErrGf3b3D6HL7b7dc7EV37xBGvVcPhGo1IpIl/zINyUkG/+1Jvf1AW6I+onFgWZbR3TeMi+NDIx39cF0eCPS0u3qHr/idtMhKTYDNIuH2lRZkpibAkuoP7XRzPDS8spauIMhyuEP7s6u1tRXr1q3DH/7wB2RnZytRAsU4WZbRPzgSCHx/2A+je/xLwD00dn8YPf3DU54Ei9drJnr5Yz/NY739Ua8vaFjk4uV+DA4HzxIZnx1iSU1A1lhYW9L8vWwpQReT5wPo+q6XnZzQSzFLEAQkGnVINOqQk3n9gVCv1wd3v+eK8B/r9Y+1NbvceLNpOGgedpxWDAT1zTekISvVOBbe/tDmGkA0mxjoRGHQaEQkSwYkSwYA15//7hnxoqt3GBpRQIpk4MlHmjcMdKJZpovTIDOFywLT/ONZFSIilWCgExGpBAOdiEglGOhERCrBQCciUgkGOhGRSig2bdHr9V8xx3XRiYjCN56Z4xk6mWKBfunSJQDgErpERNNw6dIl2Gy2oDbF1nIZGhpCQ0MD0tPTodFEdvnz+Frq+/fvR1ZW1hxVGD34eUzgZxGMn8cEtXwWXq8Xly5dwsqVK2EwGIIeU6yHbjAYwt7C7lqysrK4sNck/Dwm8LMIxs9jgho+iyt75uN4UpSISCUY6EREKsFAJyJSiagMdEmS8NWvfpX7lI7h5zGBn0Uwfh4TYuGzUGyWCxERza6o7KETEdHVGOhERCoRdTsWVVZWor6+HhcuXEBdXR2WLVumdEmKeuSRR9Da2gpRFGE0GvHd734XK1asULosxRQWFkKn00Gv1wMAtmzZgrvuukvhquZfa2srHn300cD93t5e9PX14fTp0wpWpaxXX30Vzz33HEZHR2EymbBr1y7k5OQoXdbskqPMmTNn5La2Nvnuu++WGxsblS5HcW63O3D72LFj8qZNmxSsRnn872JqTz/9tLxt2zaly1BMd3e3vHr1avm9996TZVmWDx8+LD/44IMKVzX7om7IJS8vDxaLRekyFoykpInd6vv6+iAI3JCYgnk8HtTV1eFTn/qU0qUoprm5GWlpaXA4HACAtWvX4sSJE7h8+bLClc2uqBtyoas9+eSTOHnyJGRZxs9//nOly1Hcli1bIMsybrvtNjz++OOqnqYWjuPHjyMzMxM33XST0qUoxuFwoKOjA//4xz9wyy23oK6uDgDQ3t6OlJQUhaubPVHXQ6er7dixA6+++iq++c1vYvfu3UqXo6j9+/fjyJEjOHDgAGRZRkVFhdIlKe7AgQMx3TsH/H/JVldXY9euXdi8eTM6OzshSRK0WnX1aRnoKrJp0yacOnUKXV1dSpeimPHhOJ1Oh89+9rN44403FK5IWRcvXsSZM2ewYcMGpUtR3Jo1a1BbW4uDBw+itLQUQ0NDqjspykCPYv39/Whvbw/cP378OEwmE8xms3JFKWhgYAC9vb0AAFmW8corr8T0jB8AOHToENauXYvk5GSlS1Hc+B4MPp8PP/zhD1FcXAyj0ahwVbMr6v7eePrpp3H06FF0dHTgC1/4AsxmM15++WWly1LE4OAgHnvsMQwODkIURZhMJjidzpg9MdrZ2Ymvfe1r8Hq98Pl8WLJkCZ566imly1LUoUOH8OSTTypdxoLw7LPP4o033sDIyAjuvPNObNmyRemSZh0v/SciUgkOuRARqQQDnYhIJRjoREQqwUAnIlIJBjoRkUow0Cmqtba2Yvny5RgdHZ3R6zidzsD0vlCveb3nlpeX49ChQzOqhWi6om4eOsWuwsJCdHR0QKPRBNqef/75WXntr3zlK7Py3Mlr6Rw8eBC/+c1vUFtbO6PaiMLFQKeo4nQ6sWbNmsD91tZWBashWlg45EKq0tvbiyeeeAIFBQW46667UF1dDa/XC4/Hg40bN+LFF18EAHi9XhQXF6OmpgYA8OMf//iqKwcPHDiAgoICFBQU4IUXXgi0T/XccZ/73Ofwm9/8Bv/5z3/w1FNP4c0338SqVauQl5eHf/zjH1izZk3QUE59fT02btw42x8DxSj20ElVvvWtbyEtLQ1Hjx7F4OAgvvzlL8NisaC4uBjf//73UVJSgjVr1uDo0aPw+Xx4+OGHr/lap06dwtGjR3H+/HmUlZXhgx/8YNBfB9ezZMkSbNu27aohF7PZjJMnT2Lt2rUAgCNHjjDQadawh05R5dFHH0VeXh7y8vLwyCOPBD3W0dGBP/3pT3jiiSdgNBqRmpqKBx54ILDWz7Jly/Dwww/j0UcfxQsvvIDdu3cHjcdP9V5GoxHLly/H5s2b8dvf/nbG9W/atAlHjhwBAHR3d+PEiRO47777Zvy6RAB76BRl9uzZc80x9La2NoyOjqKgoCDQ5vP5gna42rRpE6qrq/HRj34Udrv9uu81+bhFixbh3XffnXH9GzduxMc//nH09/fjd7/7HfLy8pCRkTHj1yUCGOikIllZWdDpdHjttdeuuXHBtm3bcPfdd+PEiRN4/fXXkZeXd83Xa29vx5IlSwD4vywiDd6pVr3MzMzEqlWrcOzYMbz00kv4zGc+E9FrEl0Ph1xINTIyMnDnnXfimWeeQV9fH3w+H1paWgI73R8+fBhvv/02du3ahe985zv49re/jf7+/mu+3k9+8hMMDg6iqakJBw8exCc+8YmI6klNTcXFixfh8XiC2jdu3Ijnn38e7777LtavXx/5L0p0DQx0UpXdu3djZGQEn/jEJ/DhD38YX//613Hp0iW0tbVh165dqKysREJCAjZs2ICVK1di165d13yt1atXY/369XjggQfw4IMPBg3lhOP222/HDTfcgIKCAuTn5wfa169fjwsXLmD9+vWq22CBlMX10IkUcM8996CioiLsWTNE4WAPnWie1dfXQxAE3H777UqXQirDk6JE8+hzn/sc/v3vf2P37t0QRfanaHZxyIWISCXYRSAiUgkGOhGRSjDQiYhUgoFORKQSDHQiIpVgoBMRqcT/B7e4+AxjUJ0nAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(A.values(), columns=['MSE'], index=pd.Index(A.keys(), name='Flexibility')).plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still see little evidence that using cubic or higher-order polynomial terms would leads to lower test error than simply using a quadratic fit.\n",
    "\n",
    "We saw in Section 5.3.2 that the two numbers associated with `delta` are essentially the same when LOOCV is performed. When we instead perform $k$-fold CV, then the two numbers associated with `delta` differ slightly. The first is the standard $k$-fold CV estimate, as in ( 5.3). The second is a bias-corrected version. On this data set, the two estimates are very similar to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.4 The Bootstrap\n",
    "Bootstrap means sampling with replacement. To eliminate the effect of sample size, the norm practice is to sample the same size as original dataset with replacement.\n",
    "\n",
    "Bootstrap can be used in a lot of other places, such as estimating the accuracy of a linear regression model coeffcients / Conduct non-parametric testing (permutation test) / Estimate some complicated probability \n",
    "\n",
    "We illustrate the use of the bootstrap in the simple example of Section 5.2, as well as on an example involving estimating the accuracy of the linear regression model on the `Auto` data set.\n",
    "\n",
    "### Estimating the Accuracy of a Statistic of Interest\n",
    "\n",
    "One of the great advantages of the bootstrap approach is that it can be applied in almost all situations. No complicated mathematical calculations are required. Performing a bootstrap analysis in `python` entails only two steps. First, we must create a function that computes the statistic of interest. Second, we use the `boot_python()` function, which is defined below, to perform the bootstrap by repeatedly sampling observations from the data set with replacement.\n",
    "\n",
    "The `Portfolio` data set in the `ISLR2` package is simulated data of $100$ pairs of returns, generated in the fashion described in Section 5.2.\n",
    "\n",
    "To illustrate the use of the bootstrap on this data, we must first create a function, `alpha_fn()`, which takes as input the $(X,Y)$ data as well as a vector indicating which observations should be used to estimate $\\alpha$. The function then outputs the estimate for $\\alpha$ based on the selected observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Portfolio = pd.read_csv('https://raw.githubusercontent.com/tvanzyl/Sharing_ISL_python/master/data/Portfolio.csv', header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the use of the bootstrap on this data, we must first create a function, alpha_fn(), \n",
    "which takes as input the (X, Y) data as well as a vector indicating which observations should be used to estimate alpha.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_fn(data, index):\n",
    "    X = data.X.iloc[index]\n",
    "    Y = data.Y.iloc[index]\n",
    "    return (np.var(Y) - np.cov(X,Y)[0,1])/(np.var(X) + np.var(Y) - 2 * np.cov(X, Y)[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function *returns*, or outputs, an  estimate for $\\alpha$ based on applying ( 5.7) to the observations indexed by the argument `index`.\n",
    "For instance, the following command tells `python` to estimate $\\alpha$ using\n",
    "all $100$ observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5766511516104116"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_fn(Portfolio, range(0,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next command  uses the `choice()` function to randomly select\n",
    "$100$ observations from the range $1$ to $100$, with replacement. This is equivalent\n",
    "to constructing a new bootstrap data set and recomputing $\\hat{\\alpha}$\n",
    "based on the new data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  4,  4,  7,  8,  8,  8,  9, 10, 15, 15, 16, 17, 18, 19, 20, 21,\n",
       "       22, 22, 22, 26, 31, 31, 32, 33, 34, 34, 37, 38, 39, 39, 40, 40, 40,\n",
       "       42, 43, 43, 43, 43, 46, 46, 47, 49, 49, 50, 50, 51, 52, 52, 55, 56,\n",
       "       57, 58, 60, 61, 62, 63, 63, 66, 67, 67, 68, 68, 69, 70, 70, 70, 70,\n",
       "       72, 72, 73, 74, 75, 75, 76, 76, 79, 80, 81, 82, 82, 83, 83, 84, 85,\n",
       "       86, 87, 88, 90, 90, 90, 90, 90, 91, 95, 95, 96, 96, 97, 99])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate one set of random index with 100 elements. The array has been sorted to show there are repeat elements.\n",
    "np.sort(np.random.choice(range(0, 100), size=100, replace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.632327580798003"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recall the previous function with a random set of input. \n",
    "alpha_fn(Portfolio, np.random.choice(range(0, 100), size=100, replace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# since I am not aware of boot like function in python, I just defined an ad-hoc function called boot_python()\n",
    "def boot_python(data, input_fun, iteration):\n",
    "    n = Portfolio.shape[0]\n",
    "    idx = np.random.randint(0, n, (iteration, n))\n",
    "    stat = np.zeros(iteration)\n",
    "    for i in range(len(idx)):\n",
    "        stat[i] = input_fun(data, idx[i])\n",
    "    \n",
    "    return {'Mean': np.mean(stat), 'STD': np.std(stat)}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement a bootstrap analysis by performing this command many times, recording all of\n",
    "the corresponding estimates for $\\alpha$, and computing the resulting\n",
    "standard deviation.\n",
    "However, the `boot_python()` function automates this approach. Below we produce $R=1,000$ bootstrap estimates for $\\alpha$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Mean': 0.5811900883897445, 'STD': 0.09408713019844589}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boot_python(Portfolio, alpha_fn, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final output shows that using the original data, $\\hat{\\alpha}=0.5758$,\n",
    "and that the bootstrap estimate for ${\\rm SE}(\\hat{\\alpha})$ is $0.0897$.\n",
    "\n",
    "### Estimating the Accuracy of a Linear Regression Model\n",
    "\n",
    "The bootstrap approach can be used  to assess the\n",
    "variability of the coefficient estimates and predictions from a statistical learning method. Here we use the bootstrap approach in order to assess the variability of\n",
    "the estimates for $\\beta_0$ and $\\beta_1$, the intercept and slope terms for the linear regression model\n",
    "that uses  `horsepower` to predict `mpg` in the `Auto` data set. We will compare the estimates obtained using the bootstrap to those obtained using the formulas\n",
    "for ${\\rm SE}(\\hat{\\beta}_0)$ and ${\\rm SE}(\\hat{\\beta}_1)$ described\n",
    "in Section 3.1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard error estimates for $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ obtained using the formulas from\n",
    "Section 3.1.2 are $0.717$ for the intercept and $0.0064$ for the slope. Interestingly, these are somewhat different from the\n",
    "estimates obtained using the bootstrap.  Does this indicate a problem with the bootstrap? In fact, it suggests the opposite.  Recall that the standard formulas given in Equation 3.8 on page 66 rely on certain assumptions. For example, they depend\n",
    "on the unknown parameter $\\sigma^2$, the noise variance. We then estimate $\\sigma^2$ using the RSS. Now although the formulas for the standard errors do not rely on the linear model being correct, the estimate for $\\sigma^2$ does.\n",
    "\n",
    "We see in Figure 3.8 on page 91 that there is a non-linear relationship in the data, and so the residuals from a linear fit will be inflated, and so will $\\hat{\\sigma}^2$. Secondly, the standard formulas assume (somewhat unrealistically) that the $x_i$ are fixed, and all the variability comes from the variation in the errors $\\epsilon_i$. The bootstrap approach does not rely on any of these assumptions, and so it is likely giving a more accurate estimate of the standard errors of $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you should compute the bootstrap standard error estimates and the standard\n",
    "linear regression estimates that result from fitting the quadratic model to the data. Since this model provides a good fit to the data (Figure 3.8), there is now a better correspondence between the bootstrap estimates and the standard estimates of ${\\rm SE}(\\hat{\\beta}_0)$, ${\\rm SE}(\\hat{\\beta}_1)$ and ${\\rm SE}(\\hat{\\beta}_2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.10.4 ('jupyter')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "092633841fae5a453d59f3730329b400a8541b45bc6f2e0a3e6c2e0778ee7c3a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

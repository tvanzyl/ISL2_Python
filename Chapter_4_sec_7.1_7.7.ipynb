{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.7 Lab: Classification Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7.1 The Stock Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/terencevz/anaconda3/envs/jupyter/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import math\n",
    "from patsy import dmatrices\n",
    "import statsmodels.discrete.discrete_model as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sma\n",
    "from statsmodels.graphics.regressionplots import *\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.naive_bayes import GaussianNB as NB\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Smarket = pd.read_csv('https://raw.githubusercontent.com/tvanzyl/Sharing_ISL_python/master/data/Smarket.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "      <th>Direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>-1.055</td>\n",
       "      <td>5.010</td>\n",
       "      <td>1.1913</td>\n",
       "      <td>0.959</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>-1.055</td>\n",
       "      <td>1.2965</td>\n",
       "      <td>1.032</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>1.4112</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>1.2760</td>\n",
       "      <td>0.614</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.614</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>1.2057</td>\n",
       "      <td>0.213</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year   Lag1   Lag2   Lag3   Lag4   Lag5  Volume  Today Direction\n",
       "0  2001  0.381 -0.192 -2.624 -1.055  5.010  1.1913  0.959        Up\n",
       "1  2001  0.959  0.381 -0.192 -2.624 -1.055  1.2965  1.032        Up\n",
       "2  2001  1.032  0.959  0.381 -0.192 -2.624  1.4112 -0.623      Down\n",
       "3  2001 -0.623  1.032  0.959  0.381 -0.192  1.2760  0.614        Up\n",
       "4  2001  0.614 -0.623  1.032  0.959  0.381  1.2057  0.213        Up"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Smarket.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Year', 'Lag1', 'Lag2', 'Lag3', 'Lag4', 'Lag5', 'Volume', 'Today',\n",
       "       'Direction'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Smarket.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1250, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Smarket.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.029700</td>\n",
       "      <td>0.030596</td>\n",
       "      <td>0.033195</td>\n",
       "      <td>0.035689</td>\n",
       "      <td>0.029788</td>\n",
       "      <td>0.539006</td>\n",
       "      <td>0.030095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag1</th>\n",
       "      <td>0.029700</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.026294</td>\n",
       "      <td>-0.010803</td>\n",
       "      <td>-0.002986</td>\n",
       "      <td>-0.005675</td>\n",
       "      <td>0.040910</td>\n",
       "      <td>-0.026155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag2</th>\n",
       "      <td>0.030596</td>\n",
       "      <td>-0.026294</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.025897</td>\n",
       "      <td>-0.010854</td>\n",
       "      <td>-0.003558</td>\n",
       "      <td>-0.043383</td>\n",
       "      <td>-0.010250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag3</th>\n",
       "      <td>0.033195</td>\n",
       "      <td>-0.010803</td>\n",
       "      <td>-0.025897</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.024051</td>\n",
       "      <td>-0.018808</td>\n",
       "      <td>-0.041824</td>\n",
       "      <td>-0.002448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag4</th>\n",
       "      <td>0.035689</td>\n",
       "      <td>-0.002986</td>\n",
       "      <td>-0.010854</td>\n",
       "      <td>-0.024051</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.027084</td>\n",
       "      <td>-0.048414</td>\n",
       "      <td>-0.006900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag5</th>\n",
       "      <td>0.029788</td>\n",
       "      <td>-0.005675</td>\n",
       "      <td>-0.003558</td>\n",
       "      <td>-0.018808</td>\n",
       "      <td>-0.027084</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.022002</td>\n",
       "      <td>-0.034860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Volume</th>\n",
       "      <td>0.539006</td>\n",
       "      <td>0.040910</td>\n",
       "      <td>-0.043383</td>\n",
       "      <td>-0.041824</td>\n",
       "      <td>-0.048414</td>\n",
       "      <td>-0.022002</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Today</th>\n",
       "      <td>0.030095</td>\n",
       "      <td>-0.026155</td>\n",
       "      <td>-0.010250</td>\n",
       "      <td>-0.002448</td>\n",
       "      <td>-0.006900</td>\n",
       "      <td>-0.034860</td>\n",
       "      <td>0.014592</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Year      Lag1      Lag2      Lag3      Lag4      Lag5    Volume  \\\n",
       "Year    1.000000  0.029700  0.030596  0.033195  0.035689  0.029788  0.539006   \n",
       "Lag1    0.029700  1.000000 -0.026294 -0.010803 -0.002986 -0.005675  0.040910   \n",
       "Lag2    0.030596 -0.026294  1.000000 -0.025897 -0.010854 -0.003558 -0.043383   \n",
       "Lag3    0.033195 -0.010803 -0.025897  1.000000 -0.024051 -0.018808 -0.041824   \n",
       "Lag4    0.035689 -0.002986 -0.010854 -0.024051  1.000000 -0.027084 -0.048414   \n",
       "Lag5    0.029788 -0.005675 -0.003558 -0.018808 -0.027084  1.000000 -0.022002   \n",
       "Volume  0.539006  0.040910 -0.043383 -0.041824 -0.048414 -0.022002  1.000000   \n",
       "Today   0.030095 -0.026155 -0.010250 -0.002448 -0.006900 -0.034860  0.014592   \n",
       "\n",
       "           Today  \n",
       "Year    0.030095  \n",
       "Lag1   -0.026155  \n",
       "Lag2   -0.010250  \n",
       "Lag3   -0.002448  \n",
       "Lag4   -0.006900  \n",
       "Lag5   -0.034860  \n",
       "Volume  0.014592  \n",
       "Today   1.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for panda data frame, there is a method corr to compute pairwise correlation between numerical variables\n",
    "Smarket.corr()\n",
    "# as one would expect, the correlations between the lag variables and today’s returns are close to zero. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# take a look at volume column\n",
    "plt.plot(Smarket.iloc[:, 6])\n",
    "# or plt.plot(Smarket[['Volume']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7.2 Logistic Regression\n",
    "There are some known complications that in Sklearn about applying parameter regularization. This can be aviod to set the tuning parameter 'C' to a large number. Here to be consistent with R output, I decieded to use Statsmodels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Direction[Down]  Direction[Up]\n",
      "0                 0.0            1.0\n",
      "1                 0.0            1.0\n",
      "2                 1.0            0.0\n",
      "3                 0.0            1.0\n",
      "4                 0.0            1.0\n",
      "...               ...            ...\n",
      "1245              0.0            1.0\n",
      "1246              1.0            0.0\n",
      "1247              0.0            1.0\n",
      "1248              1.0            0.0\n",
      "1249              1.0            0.0\n",
      "\n",
      "[1250 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "y, X = dmatrices('Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume', Smarket, return_type = 'dataframe')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.691034\n",
      "         Iterations 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>Direction[Up]</td>  <th>  No. Observations:  </th>  <td>  1250</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>  1243</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     6</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Mon, 15 Aug 2022</td> <th>  Pseudo R-squ.:     </th> <td>0.002074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>13:09:55</td>     <th>  Log-Likelihood:    </th> <td> -863.79</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -865.59</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td>0.7319</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   -0.1260</td> <td>    0.241</td> <td>   -0.523</td> <td> 0.601</td> <td>   -0.598</td> <td>    0.346</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lag1</th>      <td>   -0.0731</td> <td>    0.050</td> <td>   -1.457</td> <td> 0.145</td> <td>   -0.171</td> <td>    0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lag2</th>      <td>   -0.0423</td> <td>    0.050</td> <td>   -0.845</td> <td> 0.398</td> <td>   -0.140</td> <td>    0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lag3</th>      <td>    0.0111</td> <td>    0.050</td> <td>    0.222</td> <td> 0.824</td> <td>   -0.087</td> <td>    0.109</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lag4</th>      <td>    0.0094</td> <td>    0.050</td> <td>    0.187</td> <td> 0.851</td> <td>   -0.089</td> <td>    0.107</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lag5</th>      <td>    0.0103</td> <td>    0.050</td> <td>    0.208</td> <td> 0.835</td> <td>   -0.087</td> <td>    0.107</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Volume</th>    <td>    0.1354</td> <td>    0.158</td> <td>    0.855</td> <td> 0.392</td> <td>   -0.175</td> <td>    0.446</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:          Direction[Up]   No. Observations:                 1250\n",
       "Model:                          Logit   Df Residuals:                     1243\n",
       "Method:                           MLE   Df Model:                            6\n",
       "Date:                Mon, 15 Aug 2022   Pseudo R-squ.:                0.002074\n",
       "Time:                        13:09:55   Log-Likelihood:                -863.79\n",
       "converged:                       True   LL-Null:                       -865.59\n",
       "Covariance Type:            nonrobust   LLR p-value:                    0.7319\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     -0.1260      0.241     -0.523      0.601      -0.598       0.346\n",
       "Lag1          -0.0731      0.050     -1.457      0.145      -0.171       0.025\n",
       "Lag2          -0.0423      0.050     -0.845      0.398      -0.140       0.056\n",
       "Lag3           0.0111      0.050      0.222      0.824      -0.087       0.109\n",
       "Lag4           0.0094      0.050      0.187      0.851      -0.089       0.107\n",
       "Lag5           0.0103      0.050      0.208      0.835      -0.087       0.107\n",
       "Volume         0.1354      0.158      0.855      0.392      -0.175       0.446\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since we are more interested in stock marketing up, we take the second column of y as our response variable \n",
    "# we build a model to predict whether the direction will be up. \n",
    "logit = sm.Logit(y.iloc[:,1], X)\n",
    "logit.fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.691034\n",
      "         Iterations 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Intercept   -0.126000\n",
       "Lag1        -0.073074\n",
       "Lag2        -0.042301\n",
       "Lag3         0.011085\n",
       "Lag4         0.009359\n",
       "Lag5         0.010313\n",
       "Volume       0.135441\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to extract the parameters directly\n",
    "logit.fit().params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.691034\n",
      "         Iterations 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.50708413, 0.48146788, 0.48113883, 0.51522236, 0.51078116,\n",
       "       0.50695646, 0.49265087, 0.50922916, 0.51761353, 0.48883778])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to extract the probability of the market going up for the first 10 instances\n",
    "logit.fit().predict()[0:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.691034\n",
      "         Iterations 4\n"
     ]
    }
   ],
   "source": [
    "# in order to make a prediction as to whether the market will go up or down on a particular day, \n",
    "# we must convert these predicted probabilities into class labels, Up (1) or Down (0).\n",
    "# we will do this by threshold the probability by a predefined threshold \n",
    "threshold = 0.5 \n",
    "predict_label = pd.DataFrame(np.zeros(shape=(1250,1)), columns = ['label'])\n",
    "predict_label.iloc[logit.fit().predict()>threshold] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[145, 457],\n",
       "       [141, 507]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can evalue the TRAINING result by constructing a confusion matrix \n",
    "confusion_matrix(y.iloc[:,1], predict_label.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5216\n",
      "0.5216\n"
     ]
    }
   ],
   "source": [
    "# the diagonal elements of the confusion matrix indicate correct predictions, while the off-diagonals represent incorrect predictions. \n",
    "# in this case, logistic regression correctly predicted the movement of the market 52.2% of the time.\n",
    "print(np.mean(y.iloc[:,1] == predict_label.iloc[:,0]))\n",
    "# or use the confusion matrix to compute the accuracy \n",
    "print(confusion_matrix(y.iloc[:,1], predict_label.iloc[:,0]).diagonal().sum()* 1.0 /confusion_matrix(y.iloc[:,1], predict_label.iloc[:,0]).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in order to better assess the accuracy of the logistic regression model in this setting, \n",
    "# we can fit the model using part of the data, and then examine how well it predicts the hold out data. \n",
    "# this will yield a more realistic error rate, in the sense that in practice we will be interested in our \n",
    "# model’s performance not on the data that we used to fit the model, but rather on days in the future for which the market’s movements are unknown.\n",
    "Smarket_2005 = Smarket.query('Year >= 2005')\n",
    "Smarket_train = Smarket.query('Year < 2005')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we will use the training dataset to build the logistic regression model \n",
    "y_train, X_train = dmatrices('Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume', Smarket_train, return_type = 'dataframe')\n",
    "y_test, X_test = dmatrices('Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume', Smarket_2005, return_type = 'dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.691936\n",
      "         Iterations 4\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:          Direction[Up]   No. Observations:                  998\n",
      "Model:                          Logit   Df Residuals:                      991\n",
      "Method:                           MLE   Df Model:                            6\n",
      "Date:                Mon, 15 Aug 2022   Pseudo R-squ.:                0.001562\n",
      "Time:                        13:10:03   Log-Likelihood:                -690.55\n",
      "converged:                       True   LL-Null:                       -691.63\n",
      "Covariance Type:            nonrobust   LLR p-value:                    0.9044\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.1912      0.334      0.573      0.567      -0.463       0.845\n",
      "Lag1          -0.0542      0.052     -1.046      0.295      -0.156       0.047\n",
      "Lag2          -0.0458      0.052     -0.884      0.377      -0.147       0.056\n",
      "Lag3           0.0072      0.052      0.139      0.889      -0.094       0.108\n",
      "Lag4           0.0064      0.052      0.125      0.901      -0.095       0.108\n",
      "Lag5          -0.0042      0.051     -0.083      0.934      -0.104       0.096\n",
      "Volume        -0.1163      0.240     -0.485      0.628      -0.586       0.353\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "logit = sm.Logit(y_train.iloc[:,1], X_train)\n",
    "print(logit.fit().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.691936\n",
      "         Iterations 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[77, 34],\n",
       "       [97, 44]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = logit.fit().predict(X_test)\n",
    "predict_label = pd.DataFrame(np.zeros(shape=(X_test.shape[0],1)), columns = ['label'])\n",
    "threshold = 0.5\n",
    "mark = (preds > threshold).reset_index(drop=True)\n",
    "predict_label.loc[mark] = 1\n",
    "confusion_matrix(y_test.iloc[:,1], predict_label.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4801587301587302"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get accuracy\n",
    "np.mean(y_test.iloc[:,1].reset_index(drop=True)==predict_label.iloc[:,0].reset_index(drop=True)) \n",
    "\n",
    "# note: we have trained and tested our model on two completely separate data sets: \n",
    "# training was performed using only the dates before 2005, and testing was performed \n",
    "# using only the dates in 2005. Finally, we compute the predictions for 2005 and compare \n",
    "# them to the actual movements of the market over that time period. The results are rather \n",
    "# disappointing: the test error rate is 1 - 48% = 52 %, which is worse than random guessing \n",
    "# for a balanced data. Of course this result is not all that surprising, given that one \n",
    "# would not generally expect to be able to use previous days’ returns to predict future market performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692085\n",
      "         Iterations 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.44047619047619047"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the retrain of the model with Lag1 and Lag2 will be similar to previous steps (I will be brief here). \n",
    "y_train, X_train = dmatrices('Direction~Lag1+Lag2', Smarket_train, return_type = 'dataframe')\n",
    "y_test, X_test = dmatrices('Direction~Lag1+Lag2', Smarket_2005, return_type = 'dataframe')\n",
    "logit = sm.Logit(y_train.iloc[:,1], X_train)\n",
    "preds = logit.fit().predict(X_test)\n",
    "predict_label = pd.DataFrame(np.zeros(shape=(X_test.shape[0],1)), columns = ['label'])\n",
    "threshold = 0.5\n",
    "confusion_matrix(y_test.iloc[:,1], predict_label.iloc[:,0])\n",
    "np.mean(y_test.iloc[:,1].reset_index(drop=True)==predict_label.iloc[:,0].reset_index(drop=True)) # to get accuracy on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692085\n",
      "         Iterations 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5595238095238095"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another way to deal with logistics regression is to change the threshold value from 0.5 to others. \n",
    "# there is an example below with threshold 0.45. \n",
    "preds = logit.fit().predict(X_test)\n",
    "predict_label = pd.DataFrame(np.zeros(shape=(X_test.shape[0],1)), columns = ['label'])\n",
    "threshold = 0.45\n",
    "predict_label.loc[(preds > threshold).reset_index(drop=True)] = 1\n",
    "confusion_matrix(y_test.iloc[:,1], predict_label.iloc[:,0])\n",
    "\n",
    "# to get accuracy on validation set, we did see an improvment of the accuracy from 0.48 to 0.56\n",
    "np.mean(y_test.iloc[:,1].reset_index(drop=True)==predict_label.iloc[:,0].reset_index(drop=True)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7.3 Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we will use sklearn's implementation of LDA\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.iloc[:,1].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the training process \n",
    "sklearn_lda = LDA(n_components=1) #creating a LDA object\n",
    "lda = sklearn_lda.fit(X_train.iloc[:,1:3], y_train.iloc[:,1]) #learning the projection matrix\n",
    "X_lda = lda.transform(X_train.iloc[:,1:3]) #using the model to project X \n",
    "X_labels = lda.predict(X_train.iloc[:,1:3]) #gives you the predicted label for each sample\n",
    "X_prob = lda.predict_proba(X_train.iloc[:,1:3]) #the probability of each sample to belong to each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49017925 0.50982075]\n",
      " [0.4792185  0.5207815 ]\n",
      " [0.46681848 0.53318152]\n",
      " [0.47400107 0.52599893]\n",
      " [0.49278766 0.50721234]]\n"
     ]
    }
   ],
   "source": [
    "# testing step \n",
    "X_test_labels =lda.predict(X_test.iloc[:,1:3])\n",
    "X_test_prob = lda.predict_proba(X_test.iloc[:,1:3]) \n",
    "print(X_test_prob[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5595238095238095"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the accuracy of the test set using default threshold\n",
    "np.mean(y_test.iloc[:,1]==X_test_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5595238095238095"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's change the threshod a bit to see whether we can improve the accuracy. \n",
    "# the 2nd column of X_test_prob is the probability belongs to UP group. \n",
    "# the default value is 0.5, let us first check that. \n",
    "threshold = 0.5 \n",
    "np.mean(y_test.iloc[:,1]==(X_test_prob[:,1]>=threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5634920634920635"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.48\n",
    "np.mean(y_test.iloc[:,1]==(X_test_prob[:,1]>=threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7.4 Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# it is a little bit of annoying that QDA and LDA have minor difference in their parameter \n",
    "# set-up and function names. \n",
    "# from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5992063492063492\n"
     ]
    }
   ],
   "source": [
    "sklearn_qda = QDA(priors=None,store_covariance=True) #creating a QDA object\n",
    "qda = sklearn_qda.fit(X_train.iloc[:,1:3], y_train.iloc[:,1]) #learning the projection matrix\n",
    "X_labels = qda.predict(X_train.iloc[:,1:3]) #gives you the predicted label for each sample\n",
    "X_prob = qda.predict_proba(X_train.iloc[:,1:3]) #the probability of each sample to belong to each class\n",
    "\n",
    "X_test_labels=qda.predict(X_test.iloc[:,1:3])\n",
    "X_test_prob = qda.predict_proba(X_test.iloc[:,1:3]) \n",
    "\n",
    "print(np.mean(y_test.iloc[:,1]==X_test_labels) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_check_feature_names',\n",
       " '_check_n_features',\n",
       " '_decision_function',\n",
       " '_estimator_type',\n",
       " '_get_param_names',\n",
       " '_get_tags',\n",
       " '_more_tags',\n",
       " '_repr_html_',\n",
       " '_repr_html_inner',\n",
       " '_repr_mimebundle_',\n",
       " '_validate_data',\n",
       " 'classes_',\n",
       " 'covariance_',\n",
       " 'decision_function',\n",
       " 'feature_names_in_',\n",
       " 'fit',\n",
       " 'get_params',\n",
       " 'means_',\n",
       " 'n_features_in_',\n",
       " 'predict',\n",
       " 'predict_log_proba',\n",
       " 'predict_proba',\n",
       " 'priors',\n",
       " 'priors_',\n",
       " 'reg_param',\n",
       " 'rotations_',\n",
       " 'scalings_',\n",
       " 'score',\n",
       " 'set_params',\n",
       " 'store_covariance',\n",
       " 'tol']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# again, use dir() to explore all the information stored in lda and qda.\n",
    "dir(qda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04279022  0.03389409]\n",
      " [-0.03954635 -0.03132544]]\n",
      "[array([[ 1.50662277, -0.03924806],\n",
      "       [-0.03924806,  1.53559498]]), array([[ 1.51700576, -0.02787349],\n",
      "       [-0.02787349,  1.49026815]])]\n"
     ]
    }
   ],
   "source": [
    "print(qda.means_)\n",
    "print(qda.covariance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7.5 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import GaussianNB as NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5952380952380952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_check_X',\n",
       " '_check_feature_names',\n",
       " '_check_n_features',\n",
       " '_estimator_type',\n",
       " '_get_param_names',\n",
       " '_get_tags',\n",
       " '_joint_log_likelihood',\n",
       " '_more_tags',\n",
       " '_partial_fit',\n",
       " '_repr_html_',\n",
       " '_repr_html_inner',\n",
       " '_repr_mimebundle_',\n",
       " '_update_mean_variance',\n",
       " '_validate_data',\n",
       " 'class_count_',\n",
       " 'class_prior_',\n",
       " 'classes_',\n",
       " 'epsilon_',\n",
       " 'feature_names_in_',\n",
       " 'fit',\n",
       " 'get_params',\n",
       " 'n_features_in_',\n",
       " 'partial_fit',\n",
       " 'predict',\n",
       " 'predict_log_proba',\n",
       " 'predict_proba',\n",
       " 'priors',\n",
       " 'score',\n",
       " 'set_params',\n",
       " 'sigma_',\n",
       " 'theta_',\n",
       " 'var_',\n",
       " 'var_smoothing']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_class = NB()\n",
    "NB_class.fit(X_train.iloc[:,1:3], y_train.iloc[:,1])\n",
    "X_test_labels=NB_class.predict(X_test.iloc[:,1:3])\n",
    "X_test_prob = NB_class.predict_proba(X_test.iloc[:,1:3]) \n",
    "print(np.mean(y_test.iloc[:,1]==X_test_labels))\n",
    "\n",
    "dir(NB_class) # use dir command to check what Naive Bayes classifier has"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7.6 K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import KNeighborsClassifier as KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5158730158730159\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_check_algorithm_metric',\n",
       " '_check_feature_names',\n",
       " '_check_n_features',\n",
       " '_estimator_type',\n",
       " '_fit',\n",
       " '_fit_X',\n",
       " '_fit_method',\n",
       " '_get_param_names',\n",
       " '_get_tags',\n",
       " '_kneighbors_reduce_func',\n",
       " '_more_tags',\n",
       " '_repr_html_',\n",
       " '_repr_html_inner',\n",
       " '_repr_mimebundle_',\n",
       " '_tree',\n",
       " '_validate_data',\n",
       " '_y',\n",
       " 'algorithm',\n",
       " 'classes_',\n",
       " 'effective_metric_',\n",
       " 'effective_metric_params_',\n",
       " 'feature_names_in_',\n",
       " 'fit',\n",
       " 'get_params',\n",
       " 'kneighbors',\n",
       " 'kneighbors_graph',\n",
       " 'leaf_size',\n",
       " 'metric',\n",
       " 'metric_params',\n",
       " 'n_features_in_',\n",
       " 'n_jobs',\n",
       " 'n_neighbors',\n",
       " 'n_samples_fit_',\n",
       " 'outputs_2d_',\n",
       " 'p',\n",
       " 'predict',\n",
       " 'predict_proba',\n",
       " 'radius',\n",
       " 'score',\n",
       " 'set_params',\n",
       " 'weights']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh = KNN(n_neighbors= 4) # use n_neighbors to change the # of tune the performance of KNN\n",
    "KNN_fit = neigh.fit(X_train.iloc[:,1:3], y_train.iloc[:,1]) #learning the projection matrix\n",
    "X_test_labels=KNN_fit.predict(X_test.iloc[:,1:3])\n",
    "X_test_prob = KNN_fit.predict_proba(X_test.iloc[:,1:3]) \n",
    "print(np.mean(y_test.iloc[:,1]==X_test_labels))\n",
    "\n",
    "dir(neigh) # use dir command to check what KNN offers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7.7 Possion Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bikeshare = pd.read_csv('https://raw.githubusercontent.com/tvanzyl/Sharing_ISL_python/master/data/Bikeshare.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   season mnth  day  hr  holiday  weekday  workingday weathersit  temp  \\\n",
      "0       1  Jan    1   0        0        6           0      clear  0.24   \n",
      "1       1  Jan    1   1        0        6           0      clear  0.22   \n",
      "2       1  Jan    1   2        0        6           0      clear  0.22   \n",
      "3       1  Jan    1   3        0        6           0      clear  0.24   \n",
      "4       1  Jan    1   4        0        6           0      clear  0.24   \n",
      "\n",
      "    atemp   hum  windspeed  casual  registered  bikers  \n",
      "0  0.2879  0.81        0.0       3          13      16  \n",
      "1  0.2727  0.80        0.0       8          32      40  \n",
      "2  0.2727  0.80        0.0       5          27      32  \n",
      "3  0.2879  0.75        0.0       3          10      13  \n",
      "4  0.2879  0.75        0.0       0           1       1  \n",
      "(8645, 15)\n"
     ]
    }
   ],
   "source": [
    "print(Bikeshare.head())\n",
    "print(Bikeshare.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first build a linear regression model\n",
    "lm_bikeshare = smf.ols('bikers ~ mnth + hr + workingday + temp + weathersit', data = Bikeshare).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>bikers</td>      <th>  R-squared:         </th> <td>   0.364</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.363</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   291.1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 15 Aug 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:10:39</td>     <th>  Log-Likelihood:    </th> <td> -52635.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  8645</td>      <th>  AIC:               </th> <td>1.053e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  8627</td>      <th>  BIC:               </th> <td>1.054e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    17</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                <td></td>                   <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                     <td> -101.1052</td> <td>    7.269</td> <td>  -13.910</td> <td> 0.000</td> <td> -115.353</td> <td>  -86.857</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth[T.Aug]</th>                   <td>  -33.1123</td> <td>    6.410</td> <td>   -5.166</td> <td> 0.000</td> <td>  -45.677</td> <td>  -20.547</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth[T.Dec]</th>                   <td>   34.7242</td> <td>    5.908</td> <td>    5.878</td> <td> 0.000</td> <td>   23.144</td> <td>   46.305</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth[T.Feb]</th>                   <td>    5.3621</td> <td>    6.279</td> <td>    0.854</td> <td> 0.393</td> <td>   -6.946</td> <td>   17.670</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth[T.Jan]</th>                   <td>   15.8861</td> <td>    6.731</td> <td>    2.360</td> <td> 0.018</td> <td>    2.691</td> <td>   29.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth[T.July]</th>                  <td>  -52.1488</td> <td>    6.769</td> <td>   -7.704</td> <td> 0.000</td> <td>  -65.417</td> <td>  -38.881</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth[T.June]</th>                  <td>  -16.8499</td> <td>    6.343</td> <td>   -2.656</td> <td> 0.008</td> <td>  -29.284</td> <td>   -4.416</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth[T.March]</th>                 <td>    4.4564</td> <td>    5.906</td> <td>    0.755</td> <td> 0.451</td> <td>   -7.121</td> <td>   16.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth[T.May]</th>                   <td>   10.3534</td> <td>    5.757</td> <td>    1.798</td> <td> 0.072</td> <td>   -0.932</td> <td>   21.639</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth[T.Nov]</th>                   <td>   32.8573</td> <td>    5.705</td> <td>    5.759</td> <td> 0.000</td> <td>   21.674</td> <td>   44.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth[T.Oct]</th>                   <td>   34.6532</td> <td>    5.591</td> <td>    6.198</td> <td> 0.000</td> <td>   23.694</td> <td>   45.613</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth[T.Sept]</th>                  <td>   -3.0572</td> <td>    5.930</td> <td>   -0.516</td> <td> 0.606</td> <td>  -14.681</td> <td>    8.567</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weathersit[T.cloudy/misty]</th>    <td>  -10.5972</td> <td>    2.729</td> <td>   -3.884</td> <td> 0.000</td> <td>  -15.946</td> <td>   -5.249</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weathersit[T.heavy rain/snow]</th> <td>  -62.2356</td> <td>  106.855</td> <td>   -0.582</td> <td> 0.560</td> <td> -271.698</td> <td>  147.227</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weathersit[T.light rain/snow]</th> <td>  -58.2213</td> <td>    4.133</td> <td>  -14.088</td> <td> 0.000</td> <td>  -66.322</td> <td>  -50.120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hr</th>                            <td>    6.6441</td> <td>    0.174</td> <td>   38.286</td> <td> 0.000</td> <td>    6.304</td> <td>    6.984</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>workingday</th>                    <td>   -1.1738</td> <td>    2.489</td> <td>   -0.472</td> <td> 0.637</td> <td>   -6.053</td> <td>    3.705</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>temp</th>                          <td>  356.0133</td> <td>   13.007</td> <td>   27.370</td> <td> 0.000</td> <td>  330.516</td> <td>  381.511</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>1288.401</td> <th>  Durbin-Watson:     </th> <td>   0.571</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2017.317</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.044</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 4.113</td>  <th>  Cond. No.          </th> <td>1.26e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.26e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 bikers   R-squared:                       0.364\n",
       "Model:                            OLS   Adj. R-squared:                  0.363\n",
       "Method:                 Least Squares   F-statistic:                     291.1\n",
       "Date:                Mon, 15 Aug 2022   Prob (F-statistic):               0.00\n",
       "Time:                        13:10:39   Log-Likelihood:                -52635.\n",
       "No. Observations:                8645   AIC:                         1.053e+05\n",
       "Df Residuals:                    8627   BIC:                         1.054e+05\n",
       "Df Model:                          17                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=================================================================================================\n",
       "                                    coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------------------\n",
       "Intercept                      -101.1052      7.269    -13.910      0.000    -115.353     -86.857\n",
       "mnth[T.Aug]                     -33.1123      6.410     -5.166      0.000     -45.677     -20.547\n",
       "mnth[T.Dec]                      34.7242      5.908      5.878      0.000      23.144      46.305\n",
       "mnth[T.Feb]                       5.3621      6.279      0.854      0.393      -6.946      17.670\n",
       "mnth[T.Jan]                      15.8861      6.731      2.360      0.018       2.691      29.081\n",
       "mnth[T.July]                    -52.1488      6.769     -7.704      0.000     -65.417     -38.881\n",
       "mnth[T.June]                    -16.8499      6.343     -2.656      0.008     -29.284      -4.416\n",
       "mnth[T.March]                     4.4564      5.906      0.755      0.451      -7.121      16.034\n",
       "mnth[T.May]                      10.3534      5.757      1.798      0.072      -0.932      21.639\n",
       "mnth[T.Nov]                      32.8573      5.705      5.759      0.000      21.674      44.041\n",
       "mnth[T.Oct]                      34.6532      5.591      6.198      0.000      23.694      45.613\n",
       "mnth[T.Sept]                     -3.0572      5.930     -0.516      0.606     -14.681       8.567\n",
       "weathersit[T.cloudy/misty]      -10.5972      2.729     -3.884      0.000     -15.946      -5.249\n",
       "weathersit[T.heavy rain/snow]   -62.2356    106.855     -0.582      0.560    -271.698     147.227\n",
       "weathersit[T.light rain/snow]   -58.2213      4.133    -14.088      0.000     -66.322     -50.120\n",
       "hr                                6.6441      0.174     38.286      0.000       6.304       6.984\n",
       "workingday                       -1.1738      2.489     -0.472      0.637      -6.053       3.705\n",
       "temp                            356.0133     13.007     27.370      0.000     330.516     381.511\n",
       "==============================================================================\n",
       "Omnibus:                     1288.401   Durbin-Watson:                   0.571\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2017.317\n",
       "Skew:                           1.044   Prob(JB):                         0.00\n",
       "Kurtosis:                       4.113   Cond. No.                     1.26e+03\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.26e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the summary of the model, we may see the coefficients are different from the R output.\n",
    "# the diff in the coefficients is due to the difference in the way we chose the baseline for the catergotical variables.\n",
    "# here Python used April as the baseline month - probably due to the alphabetical order of the name of the month.\n",
    "lm_bikeshare.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106.65534360848793"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after building the model, we could do other things (i.e. plots, other statistics, RMSE etc.) to further explore the results. \n",
    "# here let us get a sense of the RMSE\n",
    "np.sqrt(((lm_bikeshare.fittedvalues - Bikeshare.bikers)**2).sum()/len(Bikeshare.bikers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us go ahead and build a possion regression model \n",
    "# instead of use .ols(), we use .glm()\n",
    "glm_bikeshare = smf.glm('bikers ~ mnth + hr + workingday + temp + weathersit', data = Bikeshare, family=sma.families.Poisson()).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>bikers</td>      <th>  No. Observations:  </th>   <td>  8645</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>   <td>  8627</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>         <td>Poisson</td>     <th>  Df Model:          </th>   <td>    17</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>          <td>Log</td>       <th>  Scale:             </th>  <td>  1.0000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td>-3.4404e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Mon, 15 Aug 2022</td> <th>  Deviance:          </th> <td>6.3504e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>13:10:42</td>     <th>  Pearson chi2:      </th>  <td>6.83e+05</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>          <td>5</td>        <th>  Pseudo R-squ. (CS):</th>   <td> 1.000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                <td></td>                   <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                     <td>    3.0118</td> <td>    0.006</td> <td>  476.777</td> <td> 0.000</td> <td>    2.999</td> <td>    3.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth[T.Aug]</th>                   <td>   -0.2288</td> <td>    0.005</td> <td>  -48.719</td> <td> 0.000</td> <td>   -0.238</td> <td>   -0.220</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth[T.Dec]</th>                   <td>    0.2981</td> <td>    0.005</td> <td>   59.511</td> <td> 0.000</td> <td>    0.288</td> <td>    0.308</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth[T.Feb]</th>                   <td>   -0.1015</td> <td>    0.006</td> <td>  -17.160</td> <td> 0.000</td> <td>   -0.113</td> <td>   -0.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth[T.Jan]</th>                   <td>   -0.1450</td> <td>    0.007</td> <td>  -21.388</td> <td> 0.000</td> <td>   -0.158</td> <td>   -0.132</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth[T.July]</th>                  <td>   -0.3777</td> <td>    0.005</td> <td>  -76.183</td> <td> 0.000</td> <td>   -0.387</td> <td>   -0.368</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth[T.June]</th>                  <td>   -0.1502</td> <td>    0.005</td> <td>  -32.493</td> <td> 0.000</td> <td>   -0.159</td> <td>   -0.141</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth[T.March]</th>                 <td>   -0.0312</td> <td>    0.005</td> <td>   -5.833</td> <td> 0.000</td> <td>   -0.042</td> <td>   -0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth[T.May]</th>                   <td>    0.0508</td> <td>    0.004</td> <td>   11.690</td> <td> 0.000</td> <td>    0.042</td> <td>    0.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth[T.Nov]</th>                   <td>    0.2845</td> <td>    0.005</td> <td>   61.782</td> <td> 0.000</td> <td>    0.276</td> <td>    0.294</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth[T.Oct]</th>                   <td>    0.2667</td> <td>    0.004</td> <td>   61.683</td> <td> 0.000</td> <td>    0.258</td> <td>    0.275</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth[T.Sept]</th>                  <td>   -0.0065</td> <td>    0.004</td> <td>   -1.473</td> <td> 0.141</td> <td>   -0.015</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weathersit[T.cloudy/misty]</th>    <td>   -0.0308</td> <td>    0.002</td> <td>  -14.233</td> <td> 0.000</td> <td>   -0.035</td> <td>   -0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weathersit[T.heavy rain/snow]</th> <td>   -0.6455</td> <td>    0.167</td> <td>   -3.871</td> <td> 0.000</td> <td>   -0.972</td> <td>   -0.319</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weathersit[T.light rain/snow]</th> <td>   -0.4728</td> <td>    0.004</td> <td> -116.934</td> <td> 0.000</td> <td>   -0.481</td> <td>   -0.465</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hr</th>                            <td>    0.0507</td> <td>    0.000</td> <td>  351.836</td> <td> 0.000</td> <td>    0.050</td> <td>    0.051</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>workingday</th>                    <td>   -0.0128</td> <td>    0.002</td> <td>   -6.573</td> <td> 0.000</td> <td>   -0.017</td> <td>   -0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>temp</th>                          <td>    2.5639</td> <td>    0.010</td> <td>  257.622</td> <td> 0.000</td> <td>    2.544</td> <td>    2.583</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:                 bikers   No. Observations:                 8645\n",
       "Model:                            GLM   Df Residuals:                     8627\n",
       "Model Family:                 Poisson   Df Model:                           17\n",
       "Link Function:                    Log   Scale:                          1.0000\n",
       "Method:                          IRLS   Log-Likelihood:            -3.4404e+05\n",
       "Date:                Mon, 15 Aug 2022   Deviance:                   6.3504e+05\n",
       "Time:                        13:10:42   Pearson chi2:                 6.83e+05\n",
       "No. Iterations:                     5   Pseudo R-squ. (CS):              1.000\n",
       "Covariance Type:            nonrobust                                         \n",
       "=================================================================================================\n",
       "                                    coef    std err          z      P>|z|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------------------\n",
       "Intercept                         3.0118      0.006    476.777      0.000       2.999       3.024\n",
       "mnth[T.Aug]                      -0.2288      0.005    -48.719      0.000      -0.238      -0.220\n",
       "mnth[T.Dec]                       0.2981      0.005     59.511      0.000       0.288       0.308\n",
       "mnth[T.Feb]                      -0.1015      0.006    -17.160      0.000      -0.113      -0.090\n",
       "mnth[T.Jan]                      -0.1450      0.007    -21.388      0.000      -0.158      -0.132\n",
       "mnth[T.July]                     -0.3777      0.005    -76.183      0.000      -0.387      -0.368\n",
       "mnth[T.June]                     -0.1502      0.005    -32.493      0.000      -0.159      -0.141\n",
       "mnth[T.March]                    -0.0312      0.005     -5.833      0.000      -0.042      -0.021\n",
       "mnth[T.May]                       0.0508      0.004     11.690      0.000       0.042       0.059\n",
       "mnth[T.Nov]                       0.2845      0.005     61.782      0.000       0.276       0.294\n",
       "mnth[T.Oct]                       0.2667      0.004     61.683      0.000       0.258       0.275\n",
       "mnth[T.Sept]                     -0.0065      0.004     -1.473      0.141      -0.015       0.002\n",
       "weathersit[T.cloudy/misty]       -0.0308      0.002    -14.233      0.000      -0.035      -0.027\n",
       "weathersit[T.heavy rain/snow]    -0.6455      0.167     -3.871      0.000      -0.972      -0.319\n",
       "weathersit[T.light rain/snow]    -0.4728      0.004   -116.934      0.000      -0.481      -0.465\n",
       "hr                                0.0507      0.000    351.836      0.000       0.050       0.051\n",
       "workingday                       -0.0128      0.002     -6.573      0.000      -0.017      -0.009\n",
       "temp                              2.5639      0.010    257.622      0.000       2.544       2.583\n",
       "=================================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm_bikeshare.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107.73434730516973"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we do another quick look at the training RMSE \n",
    "# to judge whether model is better, we would do train/validation split and check the model performance on the validation set.\n",
    "np.sqrt(((glm_bikeshare.fittedvalues - Bikeshare.bikers)**2).sum()/len(Bikeshare.bikers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7.8 An Application to Caravan Insurance Data \n",
    "This section is removed from the 2nd edition, but keep it as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Caravan = pd.read_csv('https://raw.githubusercontent.com/tvanzyl/Sharing_ISL_python/master/data/Caravan.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5822, 86)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Caravan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MOSTYPE</th>\n",
       "      <th>MAANTHUI</th>\n",
       "      <th>MGEMOMV</th>\n",
       "      <th>MGEMLEEF</th>\n",
       "      <th>MOSHOOFD</th>\n",
       "      <th>MGODRK</th>\n",
       "      <th>MGODPR</th>\n",
       "      <th>MGODOV</th>\n",
       "      <th>MGODGE</th>\n",
       "      <th>MRELGE</th>\n",
       "      <th>...</th>\n",
       "      <th>APERSONG</th>\n",
       "      <th>AGEZONG</th>\n",
       "      <th>AWAOREG</th>\n",
       "      <th>ABRAND</th>\n",
       "      <th>AZEILPL</th>\n",
       "      <th>APLEZIER</th>\n",
       "      <th>AFIETS</th>\n",
       "      <th>AINBOED</th>\n",
       "      <th>ABYSTAND</th>\n",
       "      <th>Purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MOSTYPE  MAANTHUI  MGEMOMV  MGEMLEEF  MOSHOOFD  MGODRK  MGODPR  MGODOV  \\\n",
       "0       33         1        3         2         8       0       5       1   \n",
       "1       37         1        2         2         8       1       4       1   \n",
       "2       37         1        2         2         8       0       4       2   \n",
       "3        9         1        3         3         3       2       3       2   \n",
       "4       40         1        4         2        10       1       4       1   \n",
       "\n",
       "   MGODGE  MRELGE  ...  APERSONG  AGEZONG  AWAOREG  ABRAND  AZEILPL  APLEZIER  \\\n",
       "0       3       7  ...         0        0        0       1        0         0   \n",
       "1       4       6  ...         0        0        0       1        0         0   \n",
       "2       4       3  ...         0        0        0       1        0         0   \n",
       "3       4       5  ...         0        0        0       1        0         0   \n",
       "4       4       7  ...         0        0        0       1        0         0   \n",
       "\n",
       "   AFIETS  AINBOED  ABYSTAND  Purchase  \n",
       "0       0        0         0        No  \n",
       "1       0        0         0        No  \n",
       "2       0        0         0        No  \n",
       "3       0        0         0        No  \n",
       "4       0        0         0        No  \n",
       "\n",
       "[5 rows x 86 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Caravan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MOSTYPE</th>\n",
       "      <th>MAANTHUI</th>\n",
       "      <th>MGEMOMV</th>\n",
       "      <th>MGEMLEEF</th>\n",
       "      <th>MOSHOOFD</th>\n",
       "      <th>MGODRK</th>\n",
       "      <th>MGODPR</th>\n",
       "      <th>MGODOV</th>\n",
       "      <th>MGODGE</th>\n",
       "      <th>MRELGE</th>\n",
       "      <th>...</th>\n",
       "      <th>ALEVEN</th>\n",
       "      <th>APERSONG</th>\n",
       "      <th>AGEZONG</th>\n",
       "      <th>AWAOREG</th>\n",
       "      <th>ABRAND</th>\n",
       "      <th>AZEILPL</th>\n",
       "      <th>APLEZIER</th>\n",
       "      <th>AFIETS</th>\n",
       "      <th>AINBOED</th>\n",
       "      <th>ABYSTAND</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>24.253349</td>\n",
       "      <td>1.110615</td>\n",
       "      <td>2.678805</td>\n",
       "      <td>2.991240</td>\n",
       "      <td>5.773617</td>\n",
       "      <td>0.696496</td>\n",
       "      <td>4.626932</td>\n",
       "      <td>1.069907</td>\n",
       "      <td>3.258502</td>\n",
       "      <td>6.183442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076606</td>\n",
       "      <td>0.005325</td>\n",
       "      <td>0.006527</td>\n",
       "      <td>0.004638</td>\n",
       "      <td>0.570079</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>0.006012</td>\n",
       "      <td>0.031776</td>\n",
       "      <td>0.007901</td>\n",
       "      <td>0.014256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.846706</td>\n",
       "      <td>0.405842</td>\n",
       "      <td>0.789835</td>\n",
       "      <td>0.814589</td>\n",
       "      <td>2.856760</td>\n",
       "      <td>1.003234</td>\n",
       "      <td>1.715843</td>\n",
       "      <td>1.017503</td>\n",
       "      <td>1.597647</td>\n",
       "      <td>1.909482</td>\n",
       "      <td>...</td>\n",
       "      <td>0.377569</td>\n",
       "      <td>0.072782</td>\n",
       "      <td>0.080532</td>\n",
       "      <td>0.077403</td>\n",
       "      <td>0.562058</td>\n",
       "      <td>0.022696</td>\n",
       "      <td>0.081632</td>\n",
       "      <td>0.210986</td>\n",
       "      <td>0.090463</td>\n",
       "      <td>0.119996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>41.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           MOSTYPE     MAANTHUI      MGEMOMV     MGEMLEEF     MOSHOOFD  \\\n",
       "count  5822.000000  5822.000000  5822.000000  5822.000000  5822.000000   \n",
       "mean     24.253349     1.110615     2.678805     2.991240     5.773617   \n",
       "std      12.846706     0.405842     0.789835     0.814589     2.856760   \n",
       "min       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "25%      10.000000     1.000000     2.000000     2.000000     3.000000   \n",
       "50%      30.000000     1.000000     3.000000     3.000000     7.000000   \n",
       "75%      35.000000     1.000000     3.000000     3.000000     8.000000   \n",
       "max      41.000000    10.000000     5.000000     6.000000    10.000000   \n",
       "\n",
       "            MGODRK       MGODPR       MGODOV       MGODGE       MRELGE  ...  \\\n",
       "count  5822.000000  5822.000000  5822.000000  5822.000000  5822.000000  ...   \n",
       "mean      0.696496     4.626932     1.069907     3.258502     6.183442  ...   \n",
       "std       1.003234     1.715843     1.017503     1.597647     1.909482  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       0.000000     4.000000     0.000000     2.000000     5.000000  ...   \n",
       "50%       0.000000     5.000000     1.000000     3.000000     6.000000  ...   \n",
       "75%       1.000000     6.000000     2.000000     4.000000     7.000000  ...   \n",
       "max       9.000000     9.000000     5.000000     9.000000     9.000000  ...   \n",
       "\n",
       "            ALEVEN     APERSONG      AGEZONG      AWAOREG       ABRAND  \\\n",
       "count  5822.000000  5822.000000  5822.000000  5822.000000  5822.000000   \n",
       "mean      0.076606     0.005325     0.006527     0.004638     0.570079   \n",
       "std       0.377569     0.072782     0.080532     0.077403     0.562058   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     1.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     1.000000   \n",
       "max       8.000000     1.000000     1.000000     2.000000     7.000000   \n",
       "\n",
       "           AZEILPL     APLEZIER       AFIETS      AINBOED     ABYSTAND  \n",
       "count  5822.000000  5822.000000  5822.000000  5822.000000  5822.000000  \n",
       "mean      0.000515     0.006012     0.031776     0.007901     0.014256  \n",
       "std       0.022696     0.081632     0.210986     0.090463     0.119996  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "max       1.000000     2.000000     3.000000     2.000000     2.000000  \n",
       "\n",
       "[8 rows x 85 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Caravan.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nScale of the variables matters in KNN ! The core question in KNN is how to define proper distance metric. \\nBecause the KNN classifier predicts the class of a given test observation by identifying the observations \\nthat are nearest to it, the scale of the variables matters. Any variables that are on a large scale will \\nhave a much larger effect on the distance between the observations, and hence on the KNN classifier, \\nthan variables that are on a small scale. For instance, imagine a data set that contains two variables, \\nsalary and age (measured in dollars and years, respectively). As far as KNN is concerned, \\na difference of 1,000 in salary is enormous compared to a difference of 50 years in age. \\nConsequently, salary will drive the KNN classification results, and age will have almost no effect. \\nThis is contrary to our intuition that a salary difference of 1, 000 is quite small compared to an age difference of 50 years. \\nFurthermore, the importance of scale to the KNN classifier leads to another issue: if we measured salary in Japanese yen, \\nor if we measured age in minutes, then we’d get quite different classification results from what we get \\nif these two variables are measured in dollars and years. \\n\\nA good (debatable) way to handle this problem is to standardize the data so that all standardize \\nvariables are given a mean of zero and a standard deviation of one. Then all variables will be on a comparable scale.\\nThe scale() function does just scale() this. In standardizing the data, we exclude column 86, \\nbecause that is the qualitative Purchase variable.\\n'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Scale of the variables matters in KNN ! The core question in KNN is how to define proper distance metric. \n",
    "Because the KNN classifier predicts the class of a given test observation by identifying the observations \n",
    "that are nearest to it, the scale of the variables matters. Any variables that are on a large scale will \n",
    "have a much larger effect on the distance between the observations, and hence on the KNN classifier, \n",
    "than variables that are on a small scale. For instance, imagine a data set that contains two variables, \n",
    "salary and age (measured in dollars and years, respectively). As far as KNN is concerned, \n",
    "a difference of 1,000 in salary is enormous compared to a difference of 50 years in age. \n",
    "Consequently, salary will drive the KNN classification results, and age will have almost no effect. \n",
    "This is contrary to our intuition that a salary difference of 1, 000 is quite small compared to an age difference of 50 years. \n",
    "Furthermore, the importance of scale to the KNN classifier leads to another issue: if we measured salary in Japanese yen, \n",
    "or if we measured age in minutes, then we’d get quite different classification results from what we get \n",
    "if these two variables are measured in dollars and years. \n",
    "\n",
    "A good (debatable) way to handle this problem is to standardize the data so that all standardize \n",
    "variables are given a mean of zero and a standard deviation of one. Then all variables will be on a comparable scale.\n",
    "The scale() function does just scale() this. In standardizing the data, we exclude column 86, \n",
    "because that is the qualitative Purchase variable.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_label = pd.DataFrame(np.zeros(shape=(Caravan.shape[0],1)), columns = ['label'])\n",
    "predict_label[Caravan['Purchase'] == 'Yes'] = 1\n",
    "Caravan_drop = Caravan.drop(labels='Purchase', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nI took a slightly different approach from the book. \\nThe training and testing data were splited by index. \\nThe normalization was done on the train set. \\nAfterwards, the same normalization was applied to validate test. \\nThe code might seem wordy, but it helps clear the logical flow. \\n'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "I took a slightly different approach from the book. \n",
    "The training and testing data were splited by index. \n",
    "The normalization was done on the train set. \n",
    "Afterwards, the same normalization was applied to validate test. \n",
    "The code might seem wordy, but it helps clear the logical flow. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I took a slightly different approach from the book. The training and testing data were splited by index. \n",
    "# the normalization was done on the train set. Afterwards, the same normalization was applied to validate test.  \n",
    "# the code might seem wordy, but it helps clear the logical flow. \n",
    "train_size = 1000\n",
    "train_index = range(0, train_size)\n",
    "X_validate = Caravan_drop.iloc[train_index, ]\n",
    "Y_validate = predict_label.iloc[train_index, ]\n",
    "X_train = Caravan_drop.iloc[train_size:, ]\n",
    "Y_train = predict_label.iloc[train_size:, ]\n",
    "\n",
    "\n",
    "X_train_scaled = preprocessing.scale(X_train)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_validate_scaled = scaler.transform(X_validate)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.883\n",
      "[[874  67]\n",
      " [ 50   9]]\n"
     ]
    }
   ],
   "source": [
    "# train with 1 neighbor \n",
    "n_neighbors = 1\n",
    "neigh = KNN(n_neighbors= n_neighbors) # use n_neighbors to change the # of tune the performance of KNN\n",
    "KNN_fit = neigh.fit(X_train_scaled, Y_train.iloc[:,0]) #learning the projection matrix\n",
    "X_validate_labels=KNN_fit.predict(X_validate_scaled)\n",
    "X_validate_prob = KNN_fit.predict_proba(X_validate_scaled) \n",
    "print(np.mean(Y_validate.iloc[:,0]==X_validate_labels))\n",
    "print(confusion_matrix(Y_validate.iloc[:,0], X_validate_labels))\n",
    "\n",
    "# the rest of this exercise considers all the trade-off between False postive and False negative.  \n",
    "# the concept of accuracy is NOT always the golden metric for classification problems. \n",
    "# precision and recall, sensitivity and specificity, F1 score... are all reasonable metrics to consider. \n",
    "# we will discuss more on the concept of trainning, validation and test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End of Chapter 4"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.10.4 ('jupyter')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "092633841fae5a453d59f3730329b400a8541b45bc6f2e0a3e6c2e0778ee7c3a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 Lab: Cross-Validation and the Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import pandas as pd \n",
    "import math\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.graphics.regressionplots import *\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from collections import OrderedDict\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "sns.set_style(\"ticks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.1 The Validation Set Approach\n",
    "\n",
    "We explore the use of the validation set approach in order to estimate the test error rates that result from fitting various linear models on the `Auto` data set.\n",
    "\n",
    "Before we begin, we use the `np.random.seed()` function in order to set a seed for `python`'s random number generator, this is so that the user of this lab will obtain precisely the same results as those in the discussions below. \n",
    "\n",
    "It is generally a good idea to set a random seed when performing an analysis such as cross-validation that contains an\n",
    "element of randomness, so that the results obtained can be reproduced precisely at a later time.\n",
    "\n",
    "We begin by using the `choice()` function to split the set of observations into two halves, by selecting a random subset of $196$ observations out of the original $392$ observations. We  refer to these observations as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(392, 9)\n",
      "    mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n",
      "0  18.0          8         307.0       130.0    3504          12.0    70   \n",
      "1  15.0          8         350.0       165.0    3693          11.5    70   \n",
      "2  18.0          8         318.0       150.0    3436          11.0    70   \n",
      "3  16.0          8         304.0       150.0    3433          12.0    70   \n",
      "4  17.0          8         302.0       140.0    3449          10.5    70   \n",
      "\n",
      "   origin                       name  \n",
      "0       1  chevrolet chevelle malibu  \n",
      "1       1          buick skylark 320  \n",
      "2       1         plymouth satellite  \n",
      "3       1              amc rebel sst  \n",
      "4       1                ford torino  \n"
     ]
    }
   ],
   "source": [
    "Auto = pd.read_csv('https://www.statlearning.com/s/Auto.csv', header=0, na_values='?')\n",
    "Auto = Auto.dropna().reset_index(drop=True) # drop the observation with NA values and reindex the obs from 0\n",
    "print(Auto.shape)\n",
    "print(Auto.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Here we use a shortcut in the choice command; see `np.random.choice` for details.)\n",
    "We then use the `train` to fit a linear regression using only the observations corresponding to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and record the index of train samples\n",
    "np.random.seed(1)\n",
    "train = np.random.choice(Auto.shape[0], 196, replace=False)\n",
    "select = np.in1d(range(Auto.shape[0]), train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    mpg   R-squared:                       0.620\n",
      "Model:                            OLS   Adj. R-squared:                  0.618\n",
      "Method:                 Least Squares   F-statistic:                     316.4\n",
      "Date:                Mon, 26 Sep 2022   Prob (F-statistic):           1.28e-42\n",
      "Time:                        09:10:14   Log-Likelihood:                -592.07\n",
      "No. Observations:                 196   AIC:                             1188.\n",
      "Df Residuals:                     194   BIC:                             1195.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     40.3338      1.023     39.416      0.000      38.316      42.352\n",
      "horsepower    -0.1596      0.009    -17.788      0.000      -0.177      -0.142\n",
      "==============================================================================\n",
      "Omnibus:                        8.393   Durbin-Watson:                   1.061\n",
      "Prob(Omnibus):                  0.015   Jarque-Bera (JB):                8.787\n",
      "Skew:                           0.516   Prob(JB):                       0.0124\n",
      "Kurtosis:                       2.899   Cond. No.                         328.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# start to build the model\n",
    "lm = smf.ols ('mpg~horsepower', data = Auto[select]).fit()\n",
    "print(lm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we choose a different training set instead, then we will obtain somewhat different errors on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now  use\n",
    " the `predict()` function to estimate the response for all $392$ observations,  and\n",
    " we  use the `mean()` function to calculate the MSE of the $196$ observations in the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Test error for 1st order model --------\n",
      "23.361902892587235\n"
     ]
    }
   ],
   "source": [
    "# to follow the book, get prediction for all the observations in the dataset\n",
    "# here we use ~ select to exclude the result of the training samples\n",
    "preds = lm.predict(Auto)\n",
    "square_error = (Auto['mpg'] - preds)**2\n",
    "print('-------- Test error for 1st order model --------')\n",
    "print(np.mean(square_error[~select]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the estimated test MSE for the linear regression fit is $23.36$. We can use the `I(X**2.0)` and `I(X**3.0)` function to estimate the test error for the quadratic and cubic regressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Test error for 2nd order--------\n",
      "20.25269085835005\n"
     ]
    }
   ],
   "source": [
    "# build a model with 2nd order of features  \n",
    "lm2 = smf.ols ('mpg~horsepower + I(horsepower ** 2.0)', data = Auto[select]).fit()\n",
    "preds = lm2.predict(Auto)\n",
    "square_error = (Auto['mpg'] - preds)**2\n",
    "print('--------Test error for 2nd order--------')\n",
    "print(square_error[~select].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Test rror for 3rd order--------\n",
      "20.325609365773584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\nThese results are consistent with our previous findings: a model that predicts mpg using a quadratic function of \\nhorsepower performs better than a model that involves only a linear function of horsepower, \\nand there is little evidence in favor of a model that uses a cubic function of horsepower.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a model with 3rd order of features  \n",
    "lm3 = smf.ols ('mpg~horsepower + I(horsepower ** 2.0) + I(horsepower ** 3.0)', data = Auto[select]).fit()\n",
    "preds = lm3.predict(Auto)\n",
    "square_error = (Auto['mpg'] - preds)**2\n",
    "print('--------Test rror for 3rd order--------')\n",
    "print(np.mean(square_error[~select]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this split of the observations into a training set and a validation set, we find that the validation set error rates for the models with linear, quadratic, and cubic terms are $23.36$, $20.25$, and $20.32$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the summmary for 3rd order regression, the coefficient of the 3rd order term is not statistically significant. We will use this as Supporting evidence for the above claim. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    mpg   R-squared:                       0.722\n",
      "Model:                            OLS   Adj. R-squared:                  0.717\n",
      "Method:                 Least Squares   F-statistic:                     165.9\n",
      "Date:                Mon, 26 Sep 2022   Prob (F-statistic):           4.60e-53\n",
      "Time:                        09:10:15   Log-Likelihood:                -561.56\n",
      "No. Observations:                 196   AIC:                             1131.\n",
      "Df Residuals:                     192   BIC:                             1144.\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "========================================================================================\n",
      "                           coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------\n",
      "Intercept               66.5200      6.310     10.541      0.000      54.073      78.967\n",
      "horsepower              -0.6868      0.162     -4.238      0.000      -1.006      -0.367\n",
      "I(horsepower ** 2.0)     0.0028      0.001      2.157      0.032       0.000       0.005\n",
      "I(horsepower ** 3.0) -3.524e-06   3.27e-06     -1.078      0.282   -9.97e-06    2.92e-06\n",
      "==============================================================================\n",
      "Omnibus:                        9.054   Durbin-Watson:                   1.328\n",
      "Prob(Omnibus):                  0.011   Jarque-Bera (JB):               15.936\n",
      "Skew:                           0.174   Prob(JB):                     0.000346\n",
      "Kurtosis:                       4.353   Cond. No.                     5.83e+07\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 5.83e+07. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "print(lm3.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are consistent with our previous findings: a model that predicts `mpg` using a quadratic function of `horsepower` performs better than a model that involves only a linear function of `horsepower`, and there is little evidence in favor of a model that uses a cubic function of `horsepower`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.2 Leave-One-Out Cross-Validation\n",
    "The LOOCV estimates only keep one sample in the validation data and use the rest of the data to train the model. This way the training model has similar dataset comparing to the model trained on entire dataset.\n",
    "\n",
    "The LOOCV estimate can be automatically computed for any generalized linear model using the `ols()` and `cross_val_score()` functions.  In the lab for Chapter 4, we used the `logit()` function to perform logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept     39.935861\n",
      "horsepower    -0.157845\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# OLS fit \n",
    "ols_fit = smf.ols ('mpg~horsepower', data = Auto).fit()\n",
    "print(ols_fit.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept     39.935861\n",
      "horsepower    -0.157845\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# GLM fit. Compare with OLS fit, the coeffs are the same\n",
    "glm_fit = smf.glm('mpg~horsepower', data = Auto).fit()\n",
    "print(glm_fit.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yield identical linear regression models. In this lab, we will  perform linear regression using\n",
    " the `glm()` function rather than the `ols()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trying CV in Python is not as easy as that in R. It will require some manual coding.\n",
    "# to use some of implemented function in Python, we use Sklearn for linear model \n",
    "# from sklearn.model_selection import KFold, cross_val_score\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.93586102117045\n",
      "[-0.15784473]\n"
     ]
    }
   ],
   "source": [
    "# let us re-train the model in sklearn\n",
    "x = pd.DataFrame(Auto.horsepower)\n",
    "y = Auto.mpg\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)\n",
    "print(model.intercept_)\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.23151351792923\n"
     ]
    }
   ],
   "source": [
    "# loo use folds equal to # of observations. We could also choose other number of folds.\n",
    "k_fold = KFold(n_splits=x.shape[0]) \n",
    "test = cross_val_score(model, x, y, cv=k_fold,  scoring = 'neg_mean_squared_error', n_jobs=-1)\n",
    "print(np.mean(-test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat this procedure for increasingly complex polynomial fits.\n",
    " To automate the process, we use the  `for` loop to initiate a  which iteratively fits polynomial regressions for polynomials of order $i=1$ to $i=10$, computes the associated cross-validation error.\n",
    " We begin by initializing the vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('1', 24.231513517929226), ('3', 19.334984064131373), ('5', 19.033216775741238), ('7', 19.12593012097048), ('9', 19.13396063366187), ('11', 19.133770655563147), ('13', 27.763421050512036), ('15', 35.29331932313156), ('17', 43.65453200631613), ('19', 60.96443125002016)])\n"
     ]
    }
   ],
   "source": [
    "# for higher order polynomial fit, we use pipline tool. \n",
    "# below shows how to fit an order 1 to 20 polynomial data and show the loo results\n",
    "# this step may take a few mins\n",
    "A = OrderedDict()\n",
    "n_split = x.shape[0]\n",
    "for porder in range(1, 21, 2):\n",
    "    model = Pipeline([('poly', PolynomialFeatures(degree=porder)), ('linear', LinearRegression())])\n",
    "    k_fold = KFold(n_splits=n_split) # loo use folds equal to # of observations\n",
    "    test = cross_val_score(model, x, y, cv=k_fold,  scoring = 'neg_mean_squared_error', n_jobs=-1)\n",
    "    A[str(porder)] = np.mean(-test)\n",
    "    \n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in Figure 5.4, we see a sharp drop in the estimated test MSE between the linear and quadratic fits, but then no clear improvement from using higher-order polynomials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.3 k-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Below we use $k=10$, a common choice for $k$, on the `Auto` data set.\n",
    "We once again set a random seed and initialize a vector in which we will store the CV errors corresponding to the polynomial fits of orders one to ten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('1', 27.439933652339857), ('3', 21.336606183328794), ('5', 20.90563054316467), ('7', 20.953366101157698), ('9', 21.036717405054862), ('11', 21.454568226381628), ('13', 30.81009074366765), ('15', 39.536725845590794), ('17', 48.27497388560606), ('19', 64.02376230983914)])\n"
     ]
    }
   ],
   "source": [
    "# K-fold validation is exactly same as LOO with different n_splits parameter setup. \n",
    "# the computation time is much shorter than that of LOOCV.\n",
    "np.random.seed(2)\n",
    "A = OrderedDict()\n",
    "n_split = 10\n",
    "for porder in range(1, 21, 2):\n",
    "    model = Pipeline([('poly', PolynomialFeatures(degree=porder)), ('linear', LinearRegression())])\n",
    "    k_fold = KFold(n_splits=n_split) \n",
    "    test = cross_val_score(model, x, y, cv = k_fold,  scoring = 'neg_mean_squared_error', n_jobs = -1)\n",
    "    A[str(porder)] = np.mean(-test)\n",
    "    \n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still see little evidence that using cubic or higher-order polynomial terms leads to lower test error than simply using a quadratic fit.\n",
    "\n",
    "We saw in Section 5.3.2 that the two numbers associated with `delta` are essentially the same when LOOCV is performed.\n",
    "When we instead perform $k$-fold CV, then the two numbers associated with `delta` differ slightly. The first is the standard $k$-fold CV estimate,\n",
    "as in ( 5.3). The second is a bias-corrected version. On this data set, the two estimates are very similar to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.4 The Bootstrap\n",
    "Bootstrap means sampling with replacement. To eliminate the effect of sample size, the norm practice is to sample the same size as original dataset with replacement.\n",
    "\n",
    "Bootstrap can be used in a lot of other places, such as estimating the accuracy of a linear regression model coeffcients / Conduct non-parametric testing (permutation test) / Estimate some complicated probability \n",
    "\n",
    "We illustrate the use of the bootstrap in the simple example of Section 5.2, as well as on an example involving estimating the accuracy of the linear regression model on the `Auto` data set.\n",
    "\n",
    "### Estimating the Accuracy of a Statistic of Interest\n",
    "\n",
    "One of the great advantages of the bootstrap approach is that it can be\n",
    "applied in almost all situations. No complicated mathematical calculations\n",
    "are required. Performing a bootstrap analysis in `python` entails only two\n",
    "steps. First, we must create a function that computes the statistic of\n",
    "interest. Second, we use the `boot_python()` function, which is defined below, to perform the bootstrap by repeatedly\n",
    "sampling observations from the data set with replacement.\n",
    "\n",
    "The `Portfolio` data set in the `ISLR2` package is simulated data of $100$ pairs of returns, generated in the fashion described in Section 5.2.\n",
    "To illustrate the use of the bootstrap on this data, we must first\n",
    "create a function, `alpha_fn()`, which takes as input the $(X,Y)$ data\n",
    "as well as a vector indicating which observations should be used to\n",
    "estimate $\\alpha$. The function then outputs the estimate for $\\alpha$\n",
    "based on the selected observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Portfolio = pd.read_csv('https://raw.githubusercontent.com/tvanzyl/Sharing_ISL_python/master/data/Portfolio.csv', header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the use of the bootstrap on this data, we must first create a function, alpha_fn(), \n",
    "which takes as input the (X, Y) data as well as a vector indicating which observations should be used to estimate alpha.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_fn(data, index):\n",
    "    X = data.X.iloc[index]\n",
    "    Y = data.Y.iloc[index]\n",
    "    return (np.var(Y) - np.cov(X,Y)[0,1])/(np.var(X) + np.var(Y) - 2 * np.cov(X, Y)[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function *returns*, or outputs, an  estimate for $\\alpha$ based on applying ( 5.7) to the observations indexed by the argument `index`.\n",
    "For instance, the following command tells `python` to estimate $\\alpha$ using\n",
    "all $100$ observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5766511516104116"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_fn(Portfolio, range(0,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next command  uses the `choice()` function to randomly select\n",
    "$100$ observations from the range $1$ to $100$, with replacement. This is equivalent\n",
    "to constructing a new bootstrap data set and recomputing $\\hat{\\alpha}$\n",
    "based on the new data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  4,  4,  7,  8,  8,  8,  9, 10, 15, 15, 16, 17, 18, 19, 20, 21,\n",
       "       22, 22, 22, 26, 31, 31, 32, 33, 34, 34, 37, 38, 39, 39, 40, 40, 40,\n",
       "       42, 43, 43, 43, 43, 46, 46, 47, 49, 49, 50, 50, 51, 52, 52, 55, 56,\n",
       "       57, 58, 60, 61, 62, 63, 63, 66, 67, 67, 68, 68, 69, 70, 70, 70, 70,\n",
       "       72, 72, 73, 74, 75, 75, 76, 76, 79, 80, 81, 82, 82, 83, 83, 84, 85,\n",
       "       86, 87, 88, 90, 90, 90, 90, 90, 91, 95, 95, 96, 96, 97, 99])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate one set of random index with 100 elements. The array has been sorted to show there are repeat elements.\n",
    "np.sort(np.random.choice(range(0, 100), size=100, replace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.632327580798003"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recall the previous function with a random set of input. \n",
    "alpha_fn(Portfolio, np.random.choice(range(0, 100), size=100, replace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# since I am not aware of boot like function in python, I just defined an ad-hoc function called boot_python()\n",
    "def boot_python(data, input_fun, iteration):\n",
    "    n = Portfolio.shape[0]\n",
    "    idx = np.random.randint(0, n, (iteration, n))\n",
    "    stat = np.zeros(iteration)\n",
    "    for i in range(len(idx)):\n",
    "        stat[i] = input_fun(data, idx[i])\n",
    "    \n",
    "    return {'Mean': np.mean(stat), 'STD': np.std(stat)}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement a bootstrap analysis by performing this command many times, recording all of\n",
    "the corresponding estimates for $\\alpha$, and computing the resulting\n",
    "standard deviation.\n",
    "However, the `boot_python()` function automates this approach. Below we produce $R=1,000$ bootstrap estimates for $\\alpha$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Mean': 0.5811900883897445, 'STD': 0.09408713019844589}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boot_python(Portfolio, alpha_fn, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final output shows that using the original data, $\\hat{\\alpha}=0.5758$,\n",
    "and that the bootstrap estimate for ${\\rm SE}(\\hat{\\alpha})$ is $0.0897$.\n",
    "\n",
    "### Estimating the Accuracy of a Linear Regression Model\n",
    "\n",
    "The bootstrap approach can be used  to assess the\n",
    "variability of the coefficient estimates and predictions from a statistical learning method. Here we use the bootstrap approach in order to assess the variability of\n",
    "the estimates for $\\beta_0$ and $\\beta_1$, the intercept and slope terms for the linear regression model\n",
    "that uses  `horsepower` to predict `mpg` in the `Auto` data set. We will compare the estimates obtained using the bootstrap to those obtained using the formulas\n",
    "for ${\\rm SE}(\\hat{\\beta}_0)$ and ${\\rm SE}(\\hat{\\beta}_1)$ described\n",
    "in Section 3.1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard error estimates for $\\hat{\\beta}_0$ and\n",
    "$\\hat{\\beta}_1$ obtained using the formulas from\n",
    "Section 3.1.2 are $0.717$ for the intercept and $0.0064$\n",
    "for the slope. Interestingly, these are somewhat different from the\n",
    "estimates obtained using the bootstrap.  Does this indicate a problem\n",
    "with the bootstrap? In fact, it suggests the opposite.  Recall that\n",
    "the standard formulas given in Equation 3.8 on page 66 rely on certain assumptions. For example, they depend\n",
    "on the unknown parameter $\\sigma^2$, the noise variance. We then estimate $\\sigma^2$\n",
    "using the RSS. Now although the formulas for the standard errors do not rely on the linear model\n",
    "being correct, the estimate for $\\sigma^2$ does.\n",
    "\n",
    "We see in Figure 3.8 on page 91 that there is a non-linear relationship in\n",
    "the data, and so the residuals from a linear fit will be inflated, and so will $\\hat{\\sigma}^2$.\n",
    "Secondly, the standard formulas assume (somewhat unrealistically) that the $x_i$ are fixed, and all the variability comes from the variation in the errors $\\epsilon_i$.\n",
    " The bootstrap approach does not rely on any of these assumptions, and so it is\n",
    "likely giving a more accurate estimate of the standard errors of\n",
    "$\\hat{\\beta}_0$ and $\\hat{\\beta}_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you should compute the bootstrap standard error estimates and the standard\n",
    "linear regression estimates that result from fitting the quadratic model to the data. Since this model provides a good fit to the data (Figure 3.8), there is now a better correspondence between the bootstrap estimates and the standard estimates of ${\\rm SE}(\\hat{\\beta}_0)$, ${\\rm SE}(\\hat{\\beta}_1)$ and ${\\rm SE}(\\hat{\\beta}_2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.10.4 ('jupyter')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "092633841fae5a453d59f3730329b400a8541b45bc6f2e0a3e6c2e0778ee7c3a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
